{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from stdcnet import STDCNet1446, STDCNet813\n",
    "# from modules.bn import InPlaceABNSync as BatchNorm2d\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "\n",
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_chan,\n",
    "                out_chan,\n",
    "                kernel_size = ks,\n",
    "                stride = stride,\n",
    "                padding = padding,\n",
    "                bias = False)\n",
    "        # self.bn = BatchNorm2d(out_chan)\n",
    "        self.bn = BatchNorm2d(out_chan)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "\n",
    "class BiSeNetOutput(nn.Module):\n",
    "    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n",
    "        super(BiSeNetOutput, self).__init__()\n",
    "        self.conv = ConvBNReLU(in_chan, mid_chan, ks=1, stride=1, padding=0)\n",
    "        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_out(x)\n",
    "        return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, BatchNorm2d):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params\n",
    "\n",
    "\n",
    "class AttentionRefinementModule(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
    "        super(AttentionRefinementModule, self).__init__()\n",
    "        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n",
    "        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n",
    "        # self.bn_atten = BatchNorm2d(out_chan)\n",
    "        self.bn_atten = BatchNorm2d(out_chan)\n",
    "\n",
    "        self.sigmoid_atten = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
    "        atten = self.conv_atten(atten)\n",
    "        atten = self.bn_atten(atten)\n",
    "        atten = self.sigmoid_atten(atten)\n",
    "        out = torch.mul(feat, atten)\n",
    "        return out\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "\n",
    "class ContextPath(nn.Module):\n",
    "    def __init__(self, backbone='CatNetSmall', pretrain_model='', use_conv_last=False, *args, **kwargs):\n",
    "        super(ContextPath, self).__init__()\n",
    "        \n",
    "        self.backbone_name = backbone\n",
    "        if backbone == 'STDCNet1446':\n",
    "            self.backbone = STDCNet1446(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n",
    "            self.arm16 = AttentionRefinementModule(512, 128)\n",
    "            inplanes = 1024\n",
    "            if use_conv_last:\n",
    "                inplanes = 1024\n",
    "            self.arm32 = AttentionRefinementModule(inplanes, 128)\n",
    "            self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
    "            self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
    "            self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n",
    "\n",
    "        elif backbone == 'STDCNet813':\n",
    "            self.backbone = STDCNet813(pretrain_model=pretrain_model, use_conv_last=use_conv_last)\n",
    "            self.arm16 = AttentionRefinementModule(512, 128)\n",
    "            inplanes = 1024\n",
    "            if use_conv_last:\n",
    "                inplanes = 1024\n",
    "            self.arm32 = AttentionRefinementModule(inplanes, 128)\n",
    "            self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
    "            self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n",
    "            self.conv_avg = ConvBNReLU(inplanes, 128, ks=1, stride=1, padding=0)\n",
    "        else:\n",
    "            print(\"backbone is not in backbone lists\")\n",
    "            exit(0)\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        H0, W0 = x.size()[2:]\n",
    "        print(\"input_size:\")\n",
    "        print(x.size())\n",
    "\n",
    "        feat2, feat4, feat8, feat16, feat32 = self.backbone(x)\n",
    "        print(\"after_backbone:feat2,feat4,feat8,feat16,feat32\")\n",
    "        print(feat2.size(),feat4.size(),feat8.size(),feat16.size(),feat32.size())\n",
    "        H8, W8 = feat8.size()[2:]\n",
    "        H16, W16 = feat16.size()[2:]\n",
    "        H32, W32 = feat32.size()[2:]\n",
    "        \n",
    "        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n",
    "        print(\"after_avg:avg\")\n",
    "        print(avg.size())\n",
    "\n",
    "        avg = self.conv_avg(avg)\n",
    "        print(\"after_conv_avg:avg\")\n",
    "        print(avg.size())\n",
    "        \n",
    "        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n",
    "        print(\"after_upsample1:avg_up\")\n",
    "        print(avg_up.size())\n",
    "\n",
    "\n",
    "        feat32_arm = self.arm32(feat32)\n",
    "        print(\"after_attention:feat32_arm\")\n",
    "        print(feat32_arm.size())\n",
    "        feat32_sum = feat32_arm + avg_up\n",
    "        print(\"after_sum:feat32_sum\")\n",
    "        print(feat32_sum.size())\n",
    "        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n",
    "        print(\"after_upsample2:feat32_up\")\n",
    "        print(feat32_up.size())\n",
    "        feat32_up = self.conv_head32(feat32_up)\n",
    "        print(\"after_conv_head32:feat32_up\")\n",
    "        print(feat32_up.size())\n",
    "\n",
    "        feat16_arm = self.arm16(feat16)\n",
    "        print(\"after_attention:feat16_arm\")\n",
    "        print(feat16_arm.size())\n",
    "        feat16_sum = feat16_arm + feat32_up\n",
    "        print(\"after_sum:feat16_sum\")\n",
    "        print(feat16_sum.size())\n",
    "        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n",
    "        print(\"after_upsample3:feat16_up\")\n",
    "        print(feat16_up.size())\n",
    "        feat16_up = self.conv_head16(feat16_up)\n",
    "        print(\"after_conv_head16:feat16_up\")\n",
    "        print(feat16_up.size())\n",
    "        print(\"contextpath end\")\n",
    "        \n",
    "        return feat2, feat4, feat8, feat16, feat16_up, feat32_up # x8, x16\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, BatchNorm2d):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params\n",
    "\n",
    "\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, in_chan, out_chan, *args, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n",
    "        self.conv1 = nn.Conv2d(out_chan,\n",
    "                out_chan//4,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                padding = 0,\n",
    "                bias = False)\n",
    "        self.conv2 = nn.Conv2d(out_chan//4,\n",
    "                out_chan,\n",
    "                kernel_size = 1,\n",
    "                stride = 1,\n",
    "                padding = 0,\n",
    "                bias = False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, fsp, fcp):\n",
    "        fcat = torch.cat([fsp, fcp], dim=1)\n",
    "        feat = self.convblk(fcat)\n",
    "        atten = F.avg_pool2d(feat, feat.size()[2:])\n",
    "        atten = self.conv1(atten)\n",
    "        atten = self.relu(atten)\n",
    "        atten = self.conv2(atten)\n",
    "        atten = self.sigmoid(atten)\n",
    "        feat_atten = torch.mul(feat, atten)\n",
    "        feat_out = feat_atten + feat\n",
    "        return feat_out\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params = [], []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                wd_params.append(module.weight)\n",
    "                if not module.bias is None:\n",
    "                    nowd_params.append(module.bias)\n",
    "            elif isinstance(module, BatchNorm2d):\n",
    "                nowd_params += list(module.parameters())\n",
    "        return wd_params, nowd_params\n",
    "\n",
    "\n",
    "class BiSeNet(nn.Module):\n",
    "    def __init__(self, backbone, n_classes, pretrain_model='', use_boundary_2=False, use_boundary_4=False, use_boundary_8=False, use_boundary_16=False, use_conv_last=False, heat_map=False, *args, **kwargs):\n",
    "        super(BiSeNet, self).__init__()\n",
    "        \n",
    "        self.use_boundary_2 = use_boundary_2\n",
    "        self.use_boundary_4 = use_boundary_4\n",
    "        self.use_boundary_8 = use_boundary_8\n",
    "        self.use_boundary_16 = use_boundary_16\n",
    "        # self.heat_map = heat_map\n",
    "        self.cp = ContextPath(backbone, pretrain_model, use_conv_last=use_conv_last)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if backbone == 'STDCNet1446':\n",
    "            conv_out_inplanes = 128\n",
    "            sp2_inplanes = 32\n",
    "            sp4_inplanes = 64\n",
    "            sp8_inplanes = 256\n",
    "            sp16_inplanes = 512\n",
    "            inplane = sp8_inplanes + conv_out_inplanes\n",
    "\n",
    "        elif backbone == 'STDCNet813':\n",
    "            conv_out_inplanes = 128\n",
    "            sp2_inplanes = 32\n",
    "            sp4_inplanes = 64\n",
    "            sp8_inplanes = 256\n",
    "            sp16_inplanes = 512\n",
    "            inplane = sp8_inplanes + conv_out_inplanes\n",
    "\n",
    "        else:\n",
    "            print(\"backbone is not in backbone lists\")\n",
    "            exit(0)\n",
    "\n",
    "        self.ffm = FeatureFusionModule(inplane, 256)\n",
    "        self.conv_out = BiSeNetOutput(256, 64, n_classes)\n",
    "        self.conv_out16 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n",
    "        self.conv_out32 = BiSeNetOutput(conv_out_inplanes, 64, n_classes)\n",
    "\n",
    "        self.conv_out_sp16 = BiSeNetOutput(sp16_inplanes, 64, 1)\n",
    "        \n",
    "        self.conv_out_sp8 = BiSeNetOutput(sp8_inplanes, 64, 1)\n",
    "        self.conv_out_sp4 = BiSeNetOutput(sp4_inplanes, 64, 1)\n",
    "        self.conv_out_sp2 = BiSeNetOutput(sp2_inplanes, 64, 1)\n",
    "        \n",
    "        self.conv_out_ajchan2 = ConvBNReLU(sp2_inplanes, n_classes, ks=3, stride=1, padding=1)\n",
    "        self.conv_out_ajchan4 = ConvBNReLU(sp4_inplanes, n_classes, ks=3, stride=1, padding=1)\n",
    "        \n",
    "#         self.CBR4 = ConvBNReLU(n_classes, n_classes, ks=3, stride=1, padding=1)\n",
    "#         self.CBR2 = ConvBNReLU(n_classes, n_classes, ks=3, stride=1, padding=1)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = x.size()[2:]\n",
    "        \n",
    "        feat_res2, feat_res4, feat_res8, feat_res16, feat_cp8, feat_cp16 = self.cp(x)\n",
    "\n",
    "        feat_out_sp2 = self.conv_out_sp2(feat_res2)\n",
    "        print(\"feat_out_sp2:\")\n",
    "        print(feat_out_sp2.size())\n",
    "\n",
    "        feat_out_sp4 = self.conv_out_sp4(feat_res4)\n",
    "        print(\"feat_out_sp4:\")\n",
    "        print(feat_out_sp4.size())\n",
    "  \n",
    "        feat_out_sp8 = self.conv_out_sp8(feat_res8)\n",
    "        print(\"feat_out_sp8:\")\n",
    "        print(feat_out_sp8.size())\n",
    "\n",
    "        feat_out_sp16 = self.conv_out_sp16(feat_res16)\n",
    "        print(\"feat_out_sp16:\")\n",
    "        print(feat_out_sp16.size())\n",
    "\n",
    "        feat_fuse = self.ffm(feat_res8, feat_cp8)\n",
    "        print(\"feat_fuse:\")\n",
    "        print(feat_fuse.size())\n",
    "\n",
    "        \n",
    "        feat_out = self.conv_out(feat_fuse)\n",
    "        print(\"feat_out:\")\n",
    "        print(feat_out.size())\n",
    "        feat_out16 = self.conv_out16(feat_cp8)\n",
    "        print(\"feat_out16:\")\n",
    "        print(feat_out16.size())\n",
    "        feat_out32 = self.conv_out32(feat_cp16)\n",
    "        print(\"feat_out32:\")\n",
    "        print(feat_out32.size())\n",
    "        \n",
    "        feat_out = F.interpolate(feat_out, (H//4, W//4), mode='bilinear', align_corners=True)\n",
    "        feat_temp8 = feat_out+self.conv_out_ajchan4(feat_res4)\n",
    "        feat_out = F.interpolate(feat_temp8, (H//2, W//2), mode='bilinear', align_corners=True)\n",
    "        feat_temp8 = feat_out+self.conv_out_ajchan2(feat_res2)\n",
    "        feat_out = F.interpolate(feat_temp8, (H, W), mode='bilinear', align_corners=True)\n",
    "        print(\"after_upsample:feat_out:\")\n",
    "        print(feat_out.size())\n",
    "        \n",
    "        feat_out16 = F.interpolate(feat_out16, (H//4, W//4), mode='bilinear', align_corners=True)\n",
    "        feat_temp16 = feat_out16+self.conv_out_ajchan4(feat_res4)\n",
    "        feat_out = F.interpolate(feat_temp16, (H//2, W//2), mode='bilinear', align_corners=True)\n",
    "        feat_temp16 = feat_out+self.conv_out_ajchan2(feat_res2)\n",
    "        feat_out = F.interpolate(feat_temp16, (H, W), mode='bilinear', align_corners=True)\n",
    "        print(\"after_upsample:feat_out16:\")\n",
    "        print(feat_out.size())\n",
    "        \n",
    "        feat_out32 = F.interpolate(feat_out32, (H//4, W//4), mode='bilinear', align_corners=True)\n",
    "        feat_temp32 = feat_out32+self.conv_out_ajchan4(feat_res4)\n",
    "        feat_out = F.interpolate(feat_temp32, (H//2, W//2), mode='bilinear', align_corners=True)\n",
    "        feat_temp32 = feat_out+self.conv_out_ajchan2(feat_res2)\n",
    "        feat_out = F.interpolate(feat_temp32, (H, W), mode='bilinear', align_corners=True)\n",
    "        print(\"after_upsample:feat_out32:\")\n",
    "        print(feat_out.size())\n",
    "\n",
    "\n",
    "        if self.use_boundary_2 and self.use_boundary_4 and self.use_boundary_8:\n",
    "            return feat_out, feat_out16, feat_out32, feat_out_sp2, feat_out_sp4, feat_out_sp8\n",
    "        \n",
    "        if (not self.use_boundary_2) and self.use_boundary_4 and self.use_boundary_8:\n",
    "            return feat_out, feat_out16, feat_out32, feat_out_sp4, feat_out_sp8\n",
    "\n",
    "        if (not self.use_boundary_2) and (not self.use_boundary_4) and self.use_boundary_8:\n",
    "            return feat_out, feat_out16, feat_out32, feat_out_sp8\n",
    "        \n",
    "        if (not self.use_boundary_2) and (not self.use_boundary_4) and (not self.use_boundary_8):\n",
    "            return feat_out, feat_out16, feat_out32\n",
    "\n",
    "    def init_weight(self):\n",
    "        for ly in self.children():\n",
    "            if isinstance(ly, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(ly.weight, a=1)\n",
    "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n",
    "\n",
    "    def get_params(self):\n",
    "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
    "        for name, child in self.named_children():\n",
    "            child_wd_params, child_nowd_params = child.get_params()\n",
    "            if isinstance(child, (FeatureFusionModule, BiSeNetOutput)):\n",
    "                lr_mul_wd_params += child_wd_params\n",
    "                lr_mul_nowd_params += child_nowd_params\n",
    "            else:\n",
    "                wd_params += child_wd_params\n",
    "                nowd_params += child_nowd_params\n",
    "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size:\n",
      "torch.Size([1, 3, 768, 1536])\n",
      "feat4:\n",
      "torch.Size([1, 64, 192, 384])\n",
      "feat8:\n",
      "torch.Size([1, 256, 96, 192])\n",
      "feat16tmp:\n",
      "torch.Size([1, 256, 48, 96])\n",
      "feat16:\n",
      "torch.Size([1, 512, 48, 96])\n",
      "feat32:\n",
      "torch.Size([1, 1024, 24, 48])\n",
      "after_backbone:feat2,feat4,feat8,feat16,feat32\n",
      "torch.Size([1, 32, 384, 768]) torch.Size([1, 64, 192, 384]) torch.Size([1, 256, 96, 192]) torch.Size([1, 512, 48, 96]) torch.Size([1, 1024, 24, 48])\n",
      "after_avg:avg\n",
      "torch.Size([1, 1024, 1, 1])\n",
      "after_conv_avg:avg\n",
      "torch.Size([1, 128, 1, 1])\n",
      "after_upsample1:avg_up\n",
      "torch.Size([1, 128, 24, 48])\n",
      "after_attention:feat32_arm\n",
      "torch.Size([1, 128, 24, 48])\n",
      "after_sum:feat32_sum\n",
      "torch.Size([1, 128, 24, 48])\n",
      "after_upsample2:feat32_up\n",
      "torch.Size([1, 128, 48, 96])\n",
      "after_conv_head32:feat32_up\n",
      "torch.Size([1, 128, 48, 96])\n",
      "after_attention:feat16_arm\n",
      "torch.Size([1, 128, 48, 96])\n",
      "after_sum:feat16_sum\n",
      "torch.Size([1, 128, 48, 96])\n",
      "after_upsample3:feat16_up\n",
      "torch.Size([1, 128, 96, 192])\n",
      "after_conv_head16:feat16_up\n",
      "torch.Size([1, 128, 96, 192])\n",
      "contextpath end\n",
      "feat_out_sp2:\n",
      "torch.Size([1, 1, 384, 768])\n",
      "feat_out_sp4:\n",
      "torch.Size([1, 1, 192, 384])\n",
      "feat_out_sp8:\n",
      "torch.Size([1, 1, 96, 192])\n",
      "feat_out_sp16:\n",
      "torch.Size([1, 1, 48, 96])\n",
      "feat_fuse:\n",
      "torch.Size([1, 256, 96, 192])\n",
      "feat_out:\n",
      "torch.Size([1, 19, 96, 192])\n",
      "feat_out16:\n",
      "torch.Size([1, 19, 96, 192])\n",
      "feat_out32:\n",
      "torch.Size([1, 19, 48, 96])\n",
      "after_upsample:feat_out:\n",
      "torch.Size([1, 19, 768, 1536])\n",
      "after_upsample:feat_out16:\n",
      "torch.Size([1, 19, 768, 1536])\n",
      "after_upsample:feat_out32:\n",
      "torch.Size([1, 19, 768, 1536])\n",
      "torch.Size([1, 19, 768, 1536])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    net = BiSeNet('STDCNet1446', 19)\n",
    "#     net.cuda()\n",
    "    net.eval()\n",
    "    in_ten = torch.randn(1, 3, 768, 1536)\n",
    "    out, out16, out32 = net(in_ten)\n",
    "    print(out.shape)\n",
    "    torch.save(net.state_dict(), 'STDCNet1446_modify_conv1layeradd.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-313bc1cd1555>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseblock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
    "        super(CatBottleneck, self).__init__()\n",
    "        self.conv_list = nn.ModuleList()\n",
    "        \n",
    "        for idx in range(block_num):\n",
    "            if idx == 0:\n",
    "                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3X(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel=3, stride=1):\n",
    "        super(Conv3X, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel, stride=stride, padding=kernel//2, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "        return out\n",
    "class Conv1X(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[[[ 0.,  1.,  2.],\n",
      "          [ 3.,  4.,  5.],\n",
      "          [ 6.,  7.,  8.],\n",
      "          [ 9., 10., 11.]],\n",
      "\n",
      "         [[12., 13., 14.],\n",
      "          [15., 16., 17.],\n",
      "          [18., 19., 20.],\n",
      "          [21., 22., 23.]]]])\n",
      "x_mean1:\n",
      "tensor([[[[ 6.,  7.,  8.],\n",
      "          [ 9., 10., 11.],\n",
      "          [12., 13., 14.],\n",
      "          [15., 16., 17.]]]])\n",
      "torch.Size([1, 1, 4, 3])\n",
      "x_mean:\n",
      "tensor(11.5000)\n"
     ]
    }
   ],
   "source": [
    "x=torch.arange(24).view(1,2,4,3)\n",
    "'''\n",
    "注意：在这里使用的时候转一下类型，否则会报RuntimeError: Can only calculate the mean of floating types. Got Long instead.的错误。\n",
    "查看了一下x元素类型是torch.int64,根据提示添加一句x=x.float()转为tensor.float32就行\n",
    "'''\n",
    "x=x.float()\n",
    "x_mean=torch.mean(x)\n",
    "# x_mean0=torch.mean(x,dim=0,keepdim=True)\n",
    "x_mean1=torch.mean(x,dim=1,keepdim=True)\n",
    "print('x:')\n",
    "print(x)\n",
    "# print('x_mean0:')\n",
    "# print(x_mean0)\n",
    "print('x_mean1:')\n",
    "print(x_mean1)\n",
    "print(x_mean1.size())\n",
    "print('x_mean:')\n",
    "print(x_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.conv.weight\n",
      "features.0.bn.weight\n",
      "features.0.bn.bias\n",
      "features.0.bn.running_mean\n",
      "features.0.bn.running_var\n",
      "features.0.bn.num_batches_tracked\n",
      "features.1.conv.weight\n",
      "features.1.bn.weight\n",
      "features.1.bn.bias\n",
      "features.1.bn.running_mean\n",
      "features.1.bn.running_var\n",
      "features.1.bn.num_batches_tracked\n",
      "features.2.conv_list.0.conv.weight\n",
      "features.2.conv_list.0.bn.weight\n",
      "features.2.conv_list.0.bn.bias\n",
      "features.2.conv_list.0.bn.running_mean\n",
      "features.2.conv_list.0.bn.running_var\n",
      "features.2.conv_list.0.bn.num_batches_tracked\n",
      "features.2.conv_list.1.conv.weight\n",
      "features.2.conv_list.1.bn.weight\n",
      "features.2.conv_list.1.bn.bias\n",
      "features.2.conv_list.1.bn.running_mean\n",
      "features.2.conv_list.1.bn.running_var\n",
      "features.2.conv_list.1.bn.num_batches_tracked\n",
      "features.2.conv_list.2.conv.weight\n",
      "features.2.conv_list.2.bn.weight\n",
      "features.2.conv_list.2.bn.bias\n",
      "features.2.conv_list.2.bn.running_mean\n",
      "features.2.conv_list.2.bn.running_var\n",
      "features.2.conv_list.2.bn.num_batches_tracked\n",
      "features.2.conv_list.3.conv.weight\n",
      "features.2.conv_list.3.bn.weight\n",
      "features.2.conv_list.3.bn.bias\n",
      "features.2.conv_list.3.bn.running_mean\n",
      "features.2.conv_list.3.bn.running_var\n",
      "features.2.conv_list.3.bn.num_batches_tracked\n",
      "features.2.avd_layer.0.weight\n",
      "features.2.avd_layer.1.weight\n",
      "features.2.avd_layer.1.bias\n",
      "features.2.avd_layer.1.running_mean\n",
      "features.2.avd_layer.1.running_var\n",
      "features.2.avd_layer.1.num_batches_tracked\n",
      "features.2.conavg_layer.0.weight\n",
      "features.2.conavg_layer.1.weight\n",
      "features.2.conavg_layer.1.bias\n",
      "features.2.conavg_layer.1.running_mean\n",
      "features.2.conavg_layer.1.running_var\n",
      "features.2.conavg_layer.1.num_batches_tracked\n",
      "features.2.conavg_layer.2.weight\n",
      "features.2.conavg_layer.3.weight\n",
      "features.2.conavg_layer.3.bias\n",
      "features.2.conavg_layer.3.running_mean\n",
      "features.2.conavg_layer.3.running_var\n",
      "features.2.conavg_layer.3.num_batches_tracked\n",
      "features.2.conavg_layer.4.weight\n",
      "features.2.conavg_layer.5.weight\n",
      "features.2.conavg_layer.5.bias\n",
      "features.2.conavg_layer.5.running_mean\n",
      "features.2.conavg_layer.5.running_var\n",
      "features.2.conavg_layer.5.num_batches_tracked\n",
      "features.3.conv_list.0.conv.weight\n",
      "features.3.conv_list.0.bn.weight\n",
      "features.3.conv_list.0.bn.bias\n",
      "features.3.conv_list.0.bn.running_mean\n",
      "features.3.conv_list.0.bn.running_var\n",
      "features.3.conv_list.0.bn.num_batches_tracked\n",
      "features.3.conv_list.1.conv.weight\n",
      "features.3.conv_list.1.bn.weight\n",
      "features.3.conv_list.1.bn.bias\n",
      "features.3.conv_list.1.bn.running_mean\n",
      "features.3.conv_list.1.bn.running_var\n",
      "features.3.conv_list.1.bn.num_batches_tracked\n",
      "features.3.conv_list.2.conv.weight\n",
      "features.3.conv_list.2.bn.weight\n",
      "features.3.conv_list.2.bn.bias\n",
      "features.3.conv_list.2.bn.running_mean\n",
      "features.3.conv_list.2.bn.running_var\n",
      "features.3.conv_list.2.bn.num_batches_tracked\n",
      "features.3.conv_list.3.conv.weight\n",
      "features.3.conv_list.3.bn.weight\n",
      "features.3.conv_list.3.bn.bias\n",
      "features.3.conv_list.3.bn.running_mean\n",
      "features.3.conv_list.3.bn.running_var\n",
      "features.3.conv_list.3.bn.num_batches_tracked\n",
      "features.3.conavg_layer.0.weight\n",
      "features.3.conavg_layer.1.weight\n",
      "features.3.conavg_layer.1.bias\n",
      "features.3.conavg_layer.1.running_mean\n",
      "features.3.conavg_layer.1.running_var\n",
      "features.3.conavg_layer.1.num_batches_tracked\n",
      "features.3.conavg_layer.2.weight\n",
      "features.3.conavg_layer.3.weight\n",
      "features.3.conavg_layer.3.bias\n",
      "features.3.conavg_layer.3.running_mean\n",
      "features.3.conavg_layer.3.running_var\n",
      "features.3.conavg_layer.3.num_batches_tracked\n",
      "features.3.conavg_layer.4.weight\n",
      "features.3.conavg_layer.5.weight\n",
      "features.3.conavg_layer.5.bias\n",
      "features.3.conavg_layer.5.running_mean\n",
      "features.3.conavg_layer.5.running_var\n",
      "features.3.conavg_layer.5.num_batches_tracked\n",
      "features.4.conv_list.0.conv.weight\n",
      "features.4.conv_list.0.bn.weight\n",
      "features.4.conv_list.0.bn.bias\n",
      "features.4.conv_list.0.bn.running_mean\n",
      "features.4.conv_list.0.bn.running_var\n",
      "features.4.conv_list.0.bn.num_batches_tracked\n",
      "features.4.conv_list.1.conv.weight\n",
      "features.4.conv_list.1.bn.weight\n",
      "features.4.conv_list.1.bn.bias\n",
      "features.4.conv_list.1.bn.running_mean\n",
      "features.4.conv_list.1.bn.running_var\n",
      "features.4.conv_list.1.bn.num_batches_tracked\n",
      "features.4.conv_list.2.conv.weight\n",
      "features.4.conv_list.2.bn.weight\n",
      "features.4.conv_list.2.bn.bias\n",
      "features.4.conv_list.2.bn.running_mean\n",
      "features.4.conv_list.2.bn.running_var\n",
      "features.4.conv_list.2.bn.num_batches_tracked\n",
      "features.4.conv_list.3.conv.weight\n",
      "features.4.conv_list.3.bn.weight\n",
      "features.4.conv_list.3.bn.bias\n",
      "features.4.conv_list.3.bn.running_mean\n",
      "features.4.conv_list.3.bn.running_var\n",
      "features.4.conv_list.3.bn.num_batches_tracked\n",
      "features.4.conavg_layer.0.weight\n",
      "features.4.conavg_layer.1.weight\n",
      "features.4.conavg_layer.1.bias\n",
      "features.4.conavg_layer.1.running_mean\n",
      "features.4.conavg_layer.1.running_var\n",
      "features.4.conavg_layer.1.num_batches_tracked\n",
      "features.4.conavg_layer.2.weight\n",
      "features.4.conavg_layer.3.weight\n",
      "features.4.conavg_layer.3.bias\n",
      "features.4.conavg_layer.3.running_mean\n",
      "features.4.conavg_layer.3.running_var\n",
      "features.4.conavg_layer.3.num_batches_tracked\n",
      "features.4.conavg_layer.4.weight\n",
      "features.4.conavg_layer.5.weight\n",
      "features.4.conavg_layer.5.bias\n",
      "features.4.conavg_layer.5.running_mean\n",
      "features.4.conavg_layer.5.running_var\n",
      "features.4.conavg_layer.5.num_batches_tracked\n",
      "features.5.conv_list.0.conv.weight\n",
      "features.5.conv_list.0.bn.weight\n",
      "features.5.conv_list.0.bn.bias\n",
      "features.5.conv_list.0.bn.running_mean\n",
      "features.5.conv_list.0.bn.running_var\n",
      "features.5.conv_list.0.bn.num_batches_tracked\n",
      "features.5.conv_list.1.conv.weight\n",
      "features.5.conv_list.1.bn.weight\n",
      "features.5.conv_list.1.bn.bias\n",
      "features.5.conv_list.1.bn.running_mean\n",
      "features.5.conv_list.1.bn.running_var\n",
      "features.5.conv_list.1.bn.num_batches_tracked\n",
      "features.5.conv_list.2.conv.weight\n",
      "features.5.conv_list.2.bn.weight\n",
      "features.5.conv_list.2.bn.bias\n",
      "features.5.conv_list.2.bn.running_mean\n",
      "features.5.conv_list.2.bn.running_var\n",
      "features.5.conv_list.2.bn.num_batches_tracked\n",
      "features.5.conv_list.3.conv.weight\n",
      "features.5.conv_list.3.bn.weight\n",
      "features.5.conv_list.3.bn.bias\n",
      "features.5.conv_list.3.bn.running_mean\n",
      "features.5.conv_list.3.bn.running_var\n",
      "features.5.conv_list.3.bn.num_batches_tracked\n",
      "features.5.conavg_layer.0.weight\n",
      "features.5.conavg_layer.1.weight\n",
      "features.5.conavg_layer.1.bias\n",
      "features.5.conavg_layer.1.running_mean\n",
      "features.5.conavg_layer.1.running_var\n",
      "features.5.conavg_layer.1.num_batches_tracked\n",
      "features.5.conavg_layer.2.weight\n",
      "features.5.conavg_layer.3.weight\n",
      "features.5.conavg_layer.3.bias\n",
      "features.5.conavg_layer.3.running_mean\n",
      "features.5.conavg_layer.3.running_var\n",
      "features.5.conavg_layer.3.num_batches_tracked\n",
      "features.5.conavg_layer.4.weight\n",
      "features.5.conavg_layer.5.weight\n",
      "features.5.conavg_layer.5.bias\n",
      "features.5.conavg_layer.5.running_mean\n",
      "features.5.conavg_layer.5.running_var\n",
      "features.5.conavg_layer.5.num_batches_tracked\n",
      "features.6.conv_list.0.conv.weight\n",
      "features.6.conv_list.0.bn.weight\n",
      "features.6.conv_list.0.bn.bias\n",
      "features.6.conv_list.0.bn.running_mean\n",
      "features.6.conv_list.0.bn.running_var\n",
      "features.6.conv_list.0.bn.num_batches_tracked\n",
      "features.6.conv_list.1.conv.weight\n",
      "features.6.conv_list.1.bn.weight\n",
      "features.6.conv_list.1.bn.bias\n",
      "features.6.conv_list.1.bn.running_mean\n",
      "features.6.conv_list.1.bn.running_var\n",
      "features.6.conv_list.1.bn.num_batches_tracked\n",
      "features.6.conv_list.2.conv.weight\n",
      "features.6.conv_list.2.bn.weight\n",
      "features.6.conv_list.2.bn.bias\n",
      "features.6.conv_list.2.bn.running_mean\n",
      "features.6.conv_list.2.bn.running_var\n",
      "features.6.conv_list.2.bn.num_batches_tracked\n",
      "features.6.conv_list.3.conv.weight\n",
      "features.6.conv_list.3.bn.weight\n",
      "features.6.conv_list.3.bn.bias\n",
      "features.6.conv_list.3.bn.running_mean\n",
      "features.6.conv_list.3.bn.running_var\n",
      "features.6.conv_list.3.bn.num_batches_tracked\n",
      "features.6.conavg_layer.0.weight\n",
      "features.6.conavg_layer.1.weight\n",
      "features.6.conavg_layer.1.bias\n",
      "features.6.conavg_layer.1.running_mean\n",
      "features.6.conavg_layer.1.running_var\n",
      "features.6.conavg_layer.1.num_batches_tracked\n",
      "features.6.conavg_layer.2.weight\n",
      "features.6.conavg_layer.3.weight\n",
      "features.6.conavg_layer.3.bias\n",
      "features.6.conavg_layer.3.running_mean\n",
      "features.6.conavg_layer.3.running_var\n",
      "features.6.conavg_layer.3.num_batches_tracked\n",
      "features.6.conavg_layer.4.weight\n",
      "features.6.conavg_layer.5.weight\n",
      "features.6.conavg_layer.5.bias\n",
      "features.6.conavg_layer.5.running_mean\n",
      "features.6.conavg_layer.5.running_var\n",
      "features.6.conavg_layer.5.num_batches_tracked\n",
      "features.7.conv_list.0.conv.weight\n",
      "features.7.conv_list.0.bn.weight\n",
      "features.7.conv_list.0.bn.bias\n",
      "features.7.conv_list.0.bn.running_mean\n",
      "features.7.conv_list.0.bn.running_var\n",
      "features.7.conv_list.0.bn.num_batches_tracked\n",
      "features.7.conv_list.1.conv.weight\n",
      "features.7.conv_list.1.bn.weight\n",
      "features.7.conv_list.1.bn.bias\n",
      "features.7.conv_list.1.bn.running_mean\n",
      "features.7.conv_list.1.bn.running_var\n",
      "features.7.conv_list.1.bn.num_batches_tracked\n",
      "features.7.conv_list.2.conv.weight\n",
      "features.7.conv_list.2.bn.weight\n",
      "features.7.conv_list.2.bn.bias\n",
      "features.7.conv_list.2.bn.running_mean\n",
      "features.7.conv_list.2.bn.running_var\n",
      "features.7.conv_list.2.bn.num_batches_tracked\n",
      "features.7.conv_list.3.conv.weight\n",
      "features.7.conv_list.3.bn.weight\n",
      "features.7.conv_list.3.bn.bias\n",
      "features.7.conv_list.3.bn.running_mean\n",
      "features.7.conv_list.3.bn.running_var\n",
      "features.7.conv_list.3.bn.num_batches_tracked\n",
      "features.7.avd_layer.0.weight\n",
      "features.7.avd_layer.1.weight\n",
      "features.7.avd_layer.1.bias\n",
      "features.7.avd_layer.1.running_mean\n",
      "features.7.avd_layer.1.running_var\n",
      "features.7.avd_layer.1.num_batches_tracked\n",
      "features.7.conavg_layer.0.weight\n",
      "features.7.conavg_layer.1.weight\n",
      "features.7.conavg_layer.1.bias\n",
      "features.7.conavg_layer.1.running_mean\n",
      "features.7.conavg_layer.1.running_var\n",
      "features.7.conavg_layer.1.num_batches_tracked\n",
      "features.7.conavg_layer.2.weight\n",
      "features.7.conavg_layer.3.weight\n",
      "features.7.conavg_layer.3.bias\n",
      "features.7.conavg_layer.3.running_mean\n",
      "features.7.conavg_layer.3.running_var\n",
      "features.7.conavg_layer.3.num_batches_tracked\n",
      "features.7.conavg_layer.4.weight\n",
      "features.7.conavg_layer.5.weight\n",
      "features.7.conavg_layer.5.bias\n",
      "features.7.conavg_layer.5.running_mean\n",
      "features.7.conavg_layer.5.running_var\n",
      "features.7.conavg_layer.5.num_batches_tracked\n",
      "features.8.conv_list.0.conv.weight\n",
      "features.8.conv_list.0.bn.weight\n",
      "features.8.conv_list.0.bn.bias\n",
      "features.8.conv_list.0.bn.running_mean\n",
      "features.8.conv_list.0.bn.running_var\n",
      "features.8.conv_list.0.bn.num_batches_tracked\n",
      "features.8.conv_list.1.conv.weight\n",
      "features.8.conv_list.1.bn.weight\n",
      "features.8.conv_list.1.bn.bias\n",
      "features.8.conv_list.1.bn.running_mean\n",
      "features.8.conv_list.1.bn.running_var\n",
      "features.8.conv_list.1.bn.num_batches_tracked\n",
      "features.8.conv_list.2.conv.weight\n",
      "features.8.conv_list.2.bn.weight\n",
      "features.8.conv_list.2.bn.bias\n",
      "features.8.conv_list.2.bn.running_mean\n",
      "features.8.conv_list.2.bn.running_var\n",
      "features.8.conv_list.2.bn.num_batches_tracked\n",
      "features.8.conv_list.3.conv.weight\n",
      "features.8.conv_list.3.bn.weight\n",
      "features.8.conv_list.3.bn.bias\n",
      "features.8.conv_list.3.bn.running_mean\n",
      "features.8.conv_list.3.bn.running_var\n",
      "features.8.conv_list.3.bn.num_batches_tracked\n",
      "features.8.conavg_layer.0.weight\n",
      "features.8.conavg_layer.1.weight\n",
      "features.8.conavg_layer.1.bias\n",
      "features.8.conavg_layer.1.running_mean\n",
      "features.8.conavg_layer.1.running_var\n",
      "features.8.conavg_layer.1.num_batches_tracked\n",
      "features.8.conavg_layer.2.weight\n",
      "features.8.conavg_layer.3.weight\n",
      "features.8.conavg_layer.3.bias\n",
      "features.8.conavg_layer.3.running_mean\n",
      "features.8.conavg_layer.3.running_var\n",
      "features.8.conavg_layer.3.num_batches_tracked\n",
      "features.8.conavg_layer.4.weight\n",
      "features.8.conavg_layer.5.weight\n",
      "features.8.conavg_layer.5.bias\n",
      "features.8.conavg_layer.5.running_mean\n",
      "features.8.conavg_layer.5.running_var\n",
      "features.8.conavg_layer.5.num_batches_tracked\n",
      "features.9.conv_list.0.conv.weight\n",
      "features.9.conv_list.0.bn.weight\n",
      "features.9.conv_list.0.bn.bias\n",
      "features.9.conv_list.0.bn.running_mean\n",
      "features.9.conv_list.0.bn.running_var\n",
      "features.9.conv_list.0.bn.num_batches_tracked\n",
      "features.9.conv_list.1.conv.weight\n",
      "features.9.conv_list.1.bn.weight\n",
      "features.9.conv_list.1.bn.bias\n",
      "features.9.conv_list.1.bn.running_mean\n",
      "features.9.conv_list.1.bn.running_var\n",
      "features.9.conv_list.1.bn.num_batches_tracked\n",
      "features.9.conv_list.2.conv.weight\n",
      "features.9.conv_list.2.bn.weight\n",
      "features.9.conv_list.2.bn.bias\n",
      "features.9.conv_list.2.bn.running_mean\n",
      "features.9.conv_list.2.bn.running_var\n",
      "features.9.conv_list.2.bn.num_batches_tracked\n",
      "features.9.conv_list.3.conv.weight\n",
      "features.9.conv_list.3.bn.weight\n",
      "features.9.conv_list.3.bn.bias\n",
      "features.9.conv_list.3.bn.running_mean\n",
      "features.9.conv_list.3.bn.running_var\n",
      "features.9.conv_list.3.bn.num_batches_tracked\n",
      "features.9.conavg_layer.0.weight\n",
      "features.9.conavg_layer.1.weight\n",
      "features.9.conavg_layer.1.bias\n",
      "features.9.conavg_layer.1.running_mean\n",
      "features.9.conavg_layer.1.running_var\n",
      "features.9.conavg_layer.1.num_batches_tracked\n",
      "features.9.conavg_layer.2.weight\n",
      "features.9.conavg_layer.3.weight\n",
      "features.9.conavg_layer.3.bias\n",
      "features.9.conavg_layer.3.running_mean\n",
      "features.9.conavg_layer.3.running_var\n",
      "features.9.conavg_layer.3.num_batches_tracked\n",
      "features.9.conavg_layer.4.weight\n",
      "features.9.conavg_layer.5.weight\n",
      "features.9.conavg_layer.5.bias\n",
      "features.9.conavg_layer.5.running_mean\n",
      "features.9.conavg_layer.5.running_var\n",
      "features.9.conavg_layer.5.num_batches_tracked\n",
      "features.10.conv_list.0.conv.weight\n",
      "features.10.conv_list.0.bn.weight\n",
      "features.10.conv_list.0.bn.bias\n",
      "features.10.conv_list.0.bn.running_mean\n",
      "features.10.conv_list.0.bn.running_var\n",
      "features.10.conv_list.0.bn.num_batches_tracked\n",
      "features.10.conv_list.1.conv.weight\n",
      "features.10.conv_list.1.bn.weight\n",
      "features.10.conv_list.1.bn.bias\n",
      "features.10.conv_list.1.bn.running_mean\n",
      "features.10.conv_list.1.bn.running_var\n",
      "features.10.conv_list.1.bn.num_batches_tracked\n",
      "features.10.conv_list.2.conv.weight\n",
      "features.10.conv_list.2.bn.weight\n",
      "features.10.conv_list.2.bn.bias\n",
      "features.10.conv_list.2.bn.running_mean\n",
      "features.10.conv_list.2.bn.running_var\n",
      "features.10.conv_list.2.bn.num_batches_tracked\n",
      "features.10.conv_list.3.conv.weight\n",
      "features.10.conv_list.3.bn.weight\n",
      "features.10.conv_list.3.bn.bias\n",
      "features.10.conv_list.3.bn.running_mean\n",
      "features.10.conv_list.3.bn.running_var\n",
      "features.10.conv_list.3.bn.num_batches_tracked\n",
      "features.10.conavg_layer.0.weight\n",
      "features.10.conavg_layer.1.weight\n",
      "features.10.conavg_layer.1.bias\n",
      "features.10.conavg_layer.1.running_mean\n",
      "features.10.conavg_layer.1.running_var\n",
      "features.10.conavg_layer.1.num_batches_tracked\n",
      "features.10.conavg_layer.2.weight\n",
      "features.10.conavg_layer.3.weight\n",
      "features.10.conavg_layer.3.bias\n",
      "features.10.conavg_layer.3.running_mean\n",
      "features.10.conavg_layer.3.running_var\n",
      "features.10.conavg_layer.3.num_batches_tracked\n",
      "features.10.conavg_layer.4.weight\n",
      "features.10.conavg_layer.5.weight\n",
      "features.10.conavg_layer.5.bias\n",
      "features.10.conavg_layer.5.running_mean\n",
      "features.10.conavg_layer.5.running_var\n",
      "features.10.conavg_layer.5.num_batches_tracked\n",
      "features.11.conv_list.0.conv.weight\n",
      "features.11.conv_list.0.bn.weight\n",
      "features.11.conv_list.0.bn.bias\n",
      "features.11.conv_list.0.bn.running_mean\n",
      "features.11.conv_list.0.bn.running_var\n",
      "features.11.conv_list.0.bn.num_batches_tracked\n",
      "features.11.conv_list.1.conv.weight\n",
      "features.11.conv_list.1.bn.weight\n",
      "features.11.conv_list.1.bn.bias\n",
      "features.11.conv_list.1.bn.running_mean\n",
      "features.11.conv_list.1.bn.running_var\n",
      "features.11.conv_list.1.bn.num_batches_tracked\n",
      "features.11.conv_list.2.conv.weight\n",
      "features.11.conv_list.2.bn.weight\n",
      "features.11.conv_list.2.bn.bias\n",
      "features.11.conv_list.2.bn.running_mean\n",
      "features.11.conv_list.2.bn.running_var\n",
      "features.11.conv_list.2.bn.num_batches_tracked\n",
      "features.11.conv_list.3.conv.weight\n",
      "features.11.conv_list.3.bn.weight\n",
      "features.11.conv_list.3.bn.bias\n",
      "features.11.conv_list.3.bn.running_mean\n",
      "features.11.conv_list.3.bn.running_var\n",
      "features.11.conv_list.3.bn.num_batches_tracked\n",
      "features.11.conavg_layer.0.weight\n",
      "features.11.conavg_layer.1.weight\n",
      "features.11.conavg_layer.1.bias\n",
      "features.11.conavg_layer.1.running_mean\n",
      "features.11.conavg_layer.1.running_var\n",
      "features.11.conavg_layer.1.num_batches_tracked\n",
      "features.11.conavg_layer.2.weight\n",
      "features.11.conavg_layer.3.weight\n",
      "features.11.conavg_layer.3.bias\n",
      "features.11.conavg_layer.3.running_mean\n",
      "features.11.conavg_layer.3.running_var\n",
      "features.11.conavg_layer.3.num_batches_tracked\n",
      "features.11.conavg_layer.4.weight\n",
      "features.11.conavg_layer.5.weight\n",
      "features.11.conavg_layer.5.bias\n",
      "features.11.conavg_layer.5.running_mean\n",
      "features.11.conavg_layer.5.running_var\n",
      "features.11.conavg_layer.5.num_batches_tracked\n",
      "features.12.conv_list.0.conv.weight\n",
      "features.12.conv_list.0.bn.weight\n",
      "features.12.conv_list.0.bn.bias\n",
      "features.12.conv_list.0.bn.running_mean\n",
      "features.12.conv_list.0.bn.running_var\n",
      "features.12.conv_list.0.bn.num_batches_tracked\n",
      "features.12.conv_list.1.conv.weight\n",
      "features.12.conv_list.1.bn.weight\n",
      "features.12.conv_list.1.bn.bias\n",
      "features.12.conv_list.1.bn.running_mean\n",
      "features.12.conv_list.1.bn.running_var\n",
      "features.12.conv_list.1.bn.num_batches_tracked\n",
      "features.12.conv_list.2.conv.weight\n",
      "features.12.conv_list.2.bn.weight\n",
      "features.12.conv_list.2.bn.bias\n",
      "features.12.conv_list.2.bn.running_mean\n",
      "features.12.conv_list.2.bn.running_var\n",
      "features.12.conv_list.2.bn.num_batches_tracked\n",
      "features.12.conv_list.3.conv.weight\n",
      "features.12.conv_list.3.bn.weight\n",
      "features.12.conv_list.3.bn.bias\n",
      "features.12.conv_list.3.bn.running_mean\n",
      "features.12.conv_list.3.bn.running_var\n",
      "features.12.conv_list.3.bn.num_batches_tracked\n",
      "features.12.conavg_layer.0.weight\n",
      "features.12.conavg_layer.1.weight\n",
      "features.12.conavg_layer.1.bias\n",
      "features.12.conavg_layer.1.running_mean\n",
      "features.12.conavg_layer.1.running_var\n",
      "features.12.conavg_layer.1.num_batches_tracked\n",
      "features.12.conavg_layer.2.weight\n",
      "features.12.conavg_layer.3.weight\n",
      "features.12.conavg_layer.3.bias\n",
      "features.12.conavg_layer.3.running_mean\n",
      "features.12.conavg_layer.3.running_var\n",
      "features.12.conavg_layer.3.num_batches_tracked\n",
      "features.12.conavg_layer.4.weight\n",
      "features.12.conavg_layer.5.weight\n",
      "features.12.conavg_layer.5.bias\n",
      "features.12.conavg_layer.5.running_mean\n",
      "features.12.conavg_layer.5.running_var\n",
      "features.12.conavg_layer.5.num_batches_tracked\n",
      "features.13.conv_list.0.conv.weight\n",
      "features.13.conv_list.0.bn.weight\n",
      "features.13.conv_list.0.bn.bias\n",
      "features.13.conv_list.0.bn.running_mean\n",
      "features.13.conv_list.0.bn.running_var\n",
      "features.13.conv_list.0.bn.num_batches_tracked\n",
      "features.13.conv_list.1.conv.weight\n",
      "features.13.conv_list.1.bn.weight\n",
      "features.13.conv_list.1.bn.bias\n",
      "features.13.conv_list.1.bn.running_mean\n",
      "features.13.conv_list.1.bn.running_var\n",
      "features.13.conv_list.1.bn.num_batches_tracked\n",
      "features.13.conv_list.2.conv.weight\n",
      "features.13.conv_list.2.bn.weight\n",
      "features.13.conv_list.2.bn.bias\n",
      "features.13.conv_list.2.bn.running_mean\n",
      "features.13.conv_list.2.bn.running_var\n",
      "features.13.conv_list.2.bn.num_batches_tracked\n",
      "features.13.conv_list.3.conv.weight\n",
      "features.13.conv_list.3.bn.weight\n",
      "features.13.conv_list.3.bn.bias\n",
      "features.13.conv_list.3.bn.running_mean\n",
      "features.13.conv_list.3.bn.running_var\n",
      "features.13.conv_list.3.bn.num_batches_tracked\n",
      "features.13.avd_layer.0.weight\n",
      "features.13.avd_layer.1.weight\n",
      "features.13.avd_layer.1.bias\n",
      "features.13.avd_layer.1.running_mean\n",
      "features.13.avd_layer.1.running_var\n",
      "features.13.avd_layer.1.num_batches_tracked\n",
      "features.13.conavg_layer.0.weight\n",
      "features.13.conavg_layer.1.weight\n",
      "features.13.conavg_layer.1.bias\n",
      "features.13.conavg_layer.1.running_mean\n",
      "features.13.conavg_layer.1.running_var\n",
      "features.13.conavg_layer.1.num_batches_tracked\n",
      "features.13.conavg_layer.2.weight\n",
      "features.13.conavg_layer.3.weight\n",
      "features.13.conavg_layer.3.bias\n",
      "features.13.conavg_layer.3.running_mean\n",
      "features.13.conavg_layer.3.running_var\n",
      "features.13.conavg_layer.3.num_batches_tracked\n",
      "features.13.conavg_layer.4.weight\n",
      "features.13.conavg_layer.5.weight\n",
      "features.13.conavg_layer.5.bias\n",
      "features.13.conavg_layer.5.running_mean\n",
      "features.13.conavg_layer.5.running_var\n",
      "features.13.conavg_layer.5.num_batches_tracked\n",
      "features.14.conv_list.0.conv.weight\n",
      "features.14.conv_list.0.bn.weight\n",
      "features.14.conv_list.0.bn.bias\n",
      "features.14.conv_list.0.bn.running_mean\n",
      "features.14.conv_list.0.bn.running_var\n",
      "features.14.conv_list.0.bn.num_batches_tracked\n",
      "features.14.conv_list.1.conv.weight\n",
      "features.14.conv_list.1.bn.weight\n",
      "features.14.conv_list.1.bn.bias\n",
      "features.14.conv_list.1.bn.running_mean\n",
      "features.14.conv_list.1.bn.running_var\n",
      "features.14.conv_list.1.bn.num_batches_tracked\n",
      "features.14.conv_list.2.conv.weight\n",
      "features.14.conv_list.2.bn.weight\n",
      "features.14.conv_list.2.bn.bias\n",
      "features.14.conv_list.2.bn.running_mean\n",
      "features.14.conv_list.2.bn.running_var\n",
      "features.14.conv_list.2.bn.num_batches_tracked\n",
      "features.14.conv_list.3.conv.weight\n",
      "features.14.conv_list.3.bn.weight\n",
      "features.14.conv_list.3.bn.bias\n",
      "features.14.conv_list.3.bn.running_mean\n",
      "features.14.conv_list.3.bn.running_var\n",
      "features.14.conv_list.3.bn.num_batches_tracked\n",
      "features.14.conavg_layer.0.weight\n",
      "features.14.conavg_layer.1.weight\n",
      "features.14.conavg_layer.1.bias\n",
      "features.14.conavg_layer.1.running_mean\n",
      "features.14.conavg_layer.1.running_var\n",
      "features.14.conavg_layer.1.num_batches_tracked\n",
      "features.14.conavg_layer.2.weight\n",
      "features.14.conavg_layer.3.weight\n",
      "features.14.conavg_layer.3.bias\n",
      "features.14.conavg_layer.3.running_mean\n",
      "features.14.conavg_layer.3.running_var\n",
      "features.14.conavg_layer.3.num_batches_tracked\n",
      "features.14.conavg_layer.4.weight\n",
      "features.14.conavg_layer.5.weight\n",
      "features.14.conavg_layer.5.bias\n",
      "features.14.conavg_layer.5.running_mean\n",
      "features.14.conavg_layer.5.running_var\n",
      "features.14.conavg_layer.5.num_batches_tracked\n",
      "features.15.conv_list.0.conv.weight\n",
      "features.15.conv_list.0.bn.weight\n",
      "features.15.conv_list.0.bn.bias\n",
      "features.15.conv_list.0.bn.running_mean\n",
      "features.15.conv_list.0.bn.running_var\n",
      "features.15.conv_list.0.bn.num_batches_tracked\n",
      "features.15.conv_list.1.conv.weight\n",
      "features.15.conv_list.1.bn.weight\n",
      "features.15.conv_list.1.bn.bias\n",
      "features.15.conv_list.1.bn.running_mean\n",
      "features.15.conv_list.1.bn.running_var\n",
      "features.15.conv_list.1.bn.num_batches_tracked\n",
      "features.15.conv_list.2.conv.weight\n",
      "features.15.conv_list.2.bn.weight\n",
      "features.15.conv_list.2.bn.bias\n",
      "features.15.conv_list.2.bn.running_mean\n",
      "features.15.conv_list.2.bn.running_var\n",
      "features.15.conv_list.2.bn.num_batches_tracked\n",
      "features.15.conv_list.3.conv.weight\n",
      "features.15.conv_list.3.bn.weight\n",
      "features.15.conv_list.3.bn.bias\n",
      "features.15.conv_list.3.bn.running_mean\n",
      "features.15.conv_list.3.bn.running_var\n",
      "features.15.conv_list.3.bn.num_batches_tracked\n",
      "features.15.conavg_layer.0.weight\n",
      "features.15.conavg_layer.1.weight\n",
      "features.15.conavg_layer.1.bias\n",
      "features.15.conavg_layer.1.running_mean\n",
      "features.15.conavg_layer.1.running_var\n",
      "features.15.conavg_layer.1.num_batches_tracked\n",
      "features.15.conavg_layer.2.weight\n",
      "features.15.conavg_layer.3.weight\n",
      "features.15.conavg_layer.3.bias\n",
      "features.15.conavg_layer.3.running_mean\n",
      "features.15.conavg_layer.3.running_var\n",
      "features.15.conavg_layer.3.num_batches_tracked\n",
      "features.15.conavg_layer.4.weight\n",
      "features.15.conavg_layer.5.weight\n",
      "features.15.conavg_layer.5.bias\n",
      "features.15.conavg_layer.5.running_mean\n",
      "features.15.conavg_layer.5.running_var\n",
      "features.15.conavg_layer.5.num_batches_tracked\n",
      "features.16.conv_list.0.conv.weight\n",
      "features.16.conv_list.0.bn.weight\n",
      "features.16.conv_list.0.bn.bias\n",
      "features.16.conv_list.0.bn.running_mean\n",
      "features.16.conv_list.0.bn.running_var\n",
      "features.16.conv_list.0.bn.num_batches_tracked\n",
      "features.16.conv_list.1.conv.weight\n",
      "features.16.conv_list.1.bn.weight\n",
      "features.16.conv_list.1.bn.bias\n",
      "features.16.conv_list.1.bn.running_mean\n",
      "features.16.conv_list.1.bn.running_var\n",
      "features.16.conv_list.1.bn.num_batches_tracked\n",
      "features.16.conv_list.2.conv.weight\n",
      "features.16.conv_list.2.bn.weight\n",
      "features.16.conv_list.2.bn.bias\n",
      "features.16.conv_list.2.bn.running_mean\n",
      "features.16.conv_list.2.bn.running_var\n",
      "features.16.conv_list.2.bn.num_batches_tracked\n",
      "features.16.conv_list.3.conv.weight\n",
      "features.16.conv_list.3.bn.weight\n",
      "features.16.conv_list.3.bn.bias\n",
      "features.16.conv_list.3.bn.running_mean\n",
      "features.16.conv_list.3.bn.running_var\n",
      "features.16.conv_list.3.bn.num_batches_tracked\n",
      "features.16.conavg_layer.0.weight\n",
      "features.16.conavg_layer.1.weight\n",
      "features.16.conavg_layer.1.bias\n",
      "features.16.conavg_layer.1.running_mean\n",
      "features.16.conavg_layer.1.running_var\n",
      "features.16.conavg_layer.1.num_batches_tracked\n",
      "features.16.conavg_layer.2.weight\n",
      "features.16.conavg_layer.3.weight\n",
      "features.16.conavg_layer.3.bias\n",
      "features.16.conavg_layer.3.running_mean\n",
      "features.16.conavg_layer.3.running_var\n",
      "features.16.conavg_layer.3.num_batches_tracked\n",
      "features.16.conavg_layer.4.weight\n",
      "features.16.conavg_layer.5.weight\n",
      "features.16.conavg_layer.5.bias\n",
      "features.16.conavg_layer.5.running_mean\n",
      "features.16.conavg_layer.5.running_var\n",
      "features.16.conavg_layer.5.num_batches_tracked\n",
      "conv_last.conv.weight\n",
      "conv_last.bn.weight\n",
      "conv_last.bn.bias\n",
      "conv_last.bn.running_mean\n",
      "conv_last.bn.running_var\n",
      "conv_last.bn.num_batches_tracked\n",
      "fc.weight\n",
      "bn.weight\n",
      "bn.bias\n",
      "bn.running_mean\n",
      "bn.running_var\n",
      "bn.num_batches_tracked\n",
      "linear.weight\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"/xiaoou/STDC-Seg-master/STDC-Seg-master/checkpoint/train_STDC2-Seg_depthwise51/pths/model_final.pth\")\n",
    "# model_resnet101.load_state_dict({k.replace('module.',''):v for k,v in torch.load(\"densenet169_rnn_fold_1_model_best_f1.pth.tar\")['state_dict'].items()})\n",
    "# state_dict = {key: value for key, value in state_dict.items() if key < 584}\n",
    "# model = BiSeNet('STDCNet1446', 19)\n",
    "new_state_dict = {}\n",
    "sub_list = list(state_dict.keys())[:673]\n",
    "for k,v in state_dict.items():\n",
    "    if k in sub_list:\n",
    "        name = k.replace('cp.backbone.','')\n",
    "        new_state_dict[name] = v\n",
    "for k,v in new_state_dict.items():\n",
    "    print(k)\n",
    "# model_resnet101.load_state_dict({k.replace('module.',''):v for k,v in torch.load(\"densenet169_rnn_fold_1_model_best_f1.pth.tar\")['state_dict'].items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.conv.weight\n",
      "features.0.bn.weight\n",
      "features.0.bn.bias\n",
      "features.0.bn.running_mean\n",
      "features.0.bn.running_var\n",
      "features.0.bn.num_batches_tracked\n",
      "features.1.conv.weight\n",
      "features.1.bn.weight\n",
      "features.1.bn.bias\n",
      "features.1.bn.running_mean\n",
      "features.1.bn.running_var\n",
      "features.1.bn.num_batches_tracked\n",
      "features.2.conv_list.0.conv.weight\n",
      "features.2.conv_list.0.bn.weight\n",
      "features.2.conv_list.0.bn.bias\n",
      "features.2.conv_list.0.bn.running_mean\n",
      "features.2.conv_list.0.bn.running_var\n",
      "features.2.conv_list.0.bn.num_batches_tracked\n",
      "features.2.conv_list.1.conv.weight\n",
      "features.2.conv_list.1.bn.weight\n",
      "features.2.conv_list.1.bn.bias\n",
      "features.2.conv_list.1.bn.running_mean\n",
      "features.2.conv_list.1.bn.running_var\n",
      "features.2.conv_list.1.bn.num_batches_tracked\n",
      "features.2.conv_list.2.conv.weight\n",
      "features.2.conv_list.2.bn.weight\n",
      "features.2.conv_list.2.bn.bias\n",
      "features.2.conv_list.2.bn.running_mean\n",
      "features.2.conv_list.2.bn.running_var\n",
      "features.2.conv_list.2.bn.num_batches_tracked\n",
      "features.2.conv_list.3.conv.weight\n",
      "features.2.conv_list.3.bn.weight\n",
      "features.2.conv_list.3.bn.bias\n",
      "features.2.conv_list.3.bn.running_mean\n",
      "features.2.conv_list.3.bn.running_var\n",
      "features.2.conv_list.3.bn.num_batches_tracked\n",
      "features.2.avd_layer.0.weight\n",
      "features.2.avd_layer.1.weight\n",
      "features.2.avd_layer.1.bias\n",
      "features.2.avd_layer.1.running_mean\n",
      "features.2.avd_layer.1.running_var\n",
      "features.2.avd_layer.1.num_batches_tracked\n",
      "features.3.conv_list.0.conv.weight\n",
      "features.3.conv_list.0.bn.weight\n",
      "features.3.conv_list.0.bn.bias\n",
      "features.3.conv_list.0.bn.running_mean\n",
      "features.3.conv_list.0.bn.running_var\n",
      "features.3.conv_list.0.bn.num_batches_tracked\n",
      "features.3.conv_list.1.conv.weight\n",
      "features.3.conv_list.1.bn.weight\n",
      "features.3.conv_list.1.bn.bias\n",
      "features.3.conv_list.1.bn.running_mean\n",
      "features.3.conv_list.1.bn.running_var\n",
      "features.3.conv_list.1.bn.num_batches_tracked\n",
      "features.3.conv_list.2.conv.weight\n",
      "features.3.conv_list.2.bn.weight\n",
      "features.3.conv_list.2.bn.bias\n",
      "features.3.conv_list.2.bn.running_mean\n",
      "features.3.conv_list.2.bn.running_var\n",
      "features.3.conv_list.2.bn.num_batches_tracked\n",
      "features.3.conv_list.3.conv.weight\n",
      "features.3.conv_list.3.bn.weight\n",
      "features.3.conv_list.3.bn.bias\n",
      "features.3.conv_list.3.bn.running_mean\n",
      "features.3.conv_list.3.bn.running_var\n",
      "features.3.conv_list.3.bn.num_batches_tracked\n",
      "features.4.conv_list.0.conv.weight\n",
      "features.4.conv_list.0.bn.weight\n",
      "features.4.conv_list.0.bn.bias\n",
      "features.4.conv_list.0.bn.running_mean\n",
      "features.4.conv_list.0.bn.running_var\n",
      "features.4.conv_list.0.bn.num_batches_tracked\n",
      "features.4.conv_list.1.conv.weight\n",
      "features.4.conv_list.1.bn.weight\n",
      "features.4.conv_list.1.bn.bias\n",
      "features.4.conv_list.1.bn.running_mean\n",
      "features.4.conv_list.1.bn.running_var\n",
      "features.4.conv_list.1.bn.num_batches_tracked\n",
      "features.4.conv_list.2.conv.weight\n",
      "features.4.conv_list.2.bn.weight\n",
      "features.4.conv_list.2.bn.bias\n",
      "features.4.conv_list.2.bn.running_mean\n",
      "features.4.conv_list.2.bn.running_var\n",
      "features.4.conv_list.2.bn.num_batches_tracked\n",
      "features.4.conv_list.3.conv.weight\n",
      "features.4.conv_list.3.bn.weight\n",
      "features.4.conv_list.3.bn.bias\n",
      "features.4.conv_list.3.bn.running_mean\n",
      "features.4.conv_list.3.bn.running_var\n",
      "features.4.conv_list.3.bn.num_batches_tracked\n",
      "features.5.conv_list.0.conv.weight\n",
      "features.5.conv_list.0.bn.weight\n",
      "features.5.conv_list.0.bn.bias\n",
      "features.5.conv_list.0.bn.running_mean\n",
      "features.5.conv_list.0.bn.running_var\n",
      "features.5.conv_list.0.bn.num_batches_tracked\n",
      "features.5.conv_list.1.conv.weight\n",
      "features.5.conv_list.1.bn.weight\n",
      "features.5.conv_list.1.bn.bias\n",
      "features.5.conv_list.1.bn.running_mean\n",
      "features.5.conv_list.1.bn.running_var\n",
      "features.5.conv_list.1.bn.num_batches_tracked\n",
      "features.5.conv_list.2.conv.weight\n",
      "features.5.conv_list.2.bn.weight\n",
      "features.5.conv_list.2.bn.bias\n",
      "features.5.conv_list.2.bn.running_mean\n",
      "features.5.conv_list.2.bn.running_var\n",
      "features.5.conv_list.2.bn.num_batches_tracked\n",
      "features.5.conv_list.3.conv.weight\n",
      "features.5.conv_list.3.bn.weight\n",
      "features.5.conv_list.3.bn.bias\n",
      "features.5.conv_list.3.bn.running_mean\n",
      "features.5.conv_list.3.bn.running_var\n",
      "features.5.conv_list.3.bn.num_batches_tracked\n",
      "features.6.conv_list.0.conv.weight\n",
      "features.6.conv_list.0.bn.weight\n",
      "features.6.conv_list.0.bn.bias\n",
      "features.6.conv_list.0.bn.running_mean\n",
      "features.6.conv_list.0.bn.running_var\n",
      "features.6.conv_list.0.bn.num_batches_tracked\n",
      "features.6.conv_list.1.conv.weight\n",
      "features.6.conv_list.1.bn.weight\n",
      "features.6.conv_list.1.bn.bias\n",
      "features.6.conv_list.1.bn.running_mean\n",
      "features.6.conv_list.1.bn.running_var\n",
      "features.6.conv_list.1.bn.num_batches_tracked\n",
      "features.6.conv_list.2.conv.weight\n",
      "features.6.conv_list.2.bn.weight\n",
      "features.6.conv_list.2.bn.bias\n",
      "features.6.conv_list.2.bn.running_mean\n",
      "features.6.conv_list.2.bn.running_var\n",
      "features.6.conv_list.2.bn.num_batches_tracked\n",
      "features.6.conv_list.3.conv.weight\n",
      "features.6.conv_list.3.bn.weight\n",
      "features.6.conv_list.3.bn.bias\n",
      "features.6.conv_list.3.bn.running_mean\n",
      "features.6.conv_list.3.bn.running_var\n",
      "features.6.conv_list.3.bn.num_batches_tracked\n",
      "features.6.avd_layer.0.weight\n",
      "features.6.avd_layer.1.weight\n",
      "features.6.avd_layer.1.bias\n",
      "features.6.avd_layer.1.running_mean\n",
      "features.6.avd_layer.1.running_var\n",
      "features.6.avd_layer.1.num_batches_tracked\n",
      "features.7.conv_list.0.conv.weight\n",
      "features.7.conv_list.0.bn.weight\n",
      "features.7.conv_list.0.bn.bias\n",
      "features.7.conv_list.0.bn.running_mean\n",
      "features.7.conv_list.0.bn.running_var\n",
      "features.7.conv_list.0.bn.num_batches_tracked\n",
      "features.7.conv_list.1.conv.weight\n",
      "features.7.conv_list.1.bn.weight\n",
      "features.7.conv_list.1.bn.bias\n",
      "features.7.conv_list.1.bn.running_mean\n",
      "features.7.conv_list.1.bn.running_var\n",
      "features.7.conv_list.1.bn.num_batches_tracked\n",
      "features.7.conv_list.2.conv.weight\n",
      "features.7.conv_list.2.bn.weight\n",
      "features.7.conv_list.2.bn.bias\n",
      "features.7.conv_list.2.bn.running_mean\n",
      "features.7.conv_list.2.bn.running_var\n",
      "features.7.conv_list.2.bn.num_batches_tracked\n",
      "features.7.conv_list.3.conv.weight\n",
      "features.7.conv_list.3.bn.weight\n",
      "features.7.conv_list.3.bn.bias\n",
      "features.7.conv_list.3.bn.running_mean\n",
      "features.7.conv_list.3.bn.running_var\n",
      "features.7.conv_list.3.bn.num_batches_tracked\n",
      "features.8.conv_list.0.conv.weight\n",
      "features.8.conv_list.0.bn.weight\n",
      "features.8.conv_list.0.bn.bias\n",
      "features.8.conv_list.0.bn.running_mean\n",
      "features.8.conv_list.0.bn.running_var\n",
      "features.8.conv_list.0.bn.num_batches_tracked\n",
      "features.8.conv_list.1.conv.weight\n",
      "features.8.conv_list.1.bn.weight\n",
      "features.8.conv_list.1.bn.bias\n",
      "features.8.conv_list.1.bn.running_mean\n",
      "features.8.conv_list.1.bn.running_var\n",
      "features.8.conv_list.1.bn.num_batches_tracked\n",
      "features.8.conv_list.2.conv.weight\n",
      "features.8.conv_list.2.bn.weight\n",
      "features.8.conv_list.2.bn.bias\n",
      "features.8.conv_list.2.bn.running_mean\n",
      "features.8.conv_list.2.bn.running_var\n",
      "features.8.conv_list.2.bn.num_batches_tracked\n",
      "features.8.conv_list.3.conv.weight\n",
      "features.8.conv_list.3.bn.weight\n",
      "features.8.conv_list.3.bn.bias\n",
      "features.8.conv_list.3.bn.running_mean\n",
      "features.8.conv_list.3.bn.running_var\n",
      "features.8.conv_list.3.bn.num_batches_tracked\n",
      "features.9.conv_list.0.conv.weight\n",
      "features.9.conv_list.0.bn.weight\n",
      "features.9.conv_list.0.bn.bias\n",
      "features.9.conv_list.0.bn.running_mean\n",
      "features.9.conv_list.0.bn.running_var\n",
      "features.9.conv_list.0.bn.num_batches_tracked\n",
      "features.9.conv_list.1.conv.weight\n",
      "features.9.conv_list.1.bn.weight\n",
      "features.9.conv_list.1.bn.bias\n",
      "features.9.conv_list.1.bn.running_mean\n",
      "features.9.conv_list.1.bn.running_var\n",
      "features.9.conv_list.1.bn.num_batches_tracked\n",
      "features.9.conv_list.2.conv.weight\n",
      "features.9.conv_list.2.bn.weight\n",
      "features.9.conv_list.2.bn.bias\n",
      "features.9.conv_list.2.bn.running_mean\n",
      "features.9.conv_list.2.bn.running_var\n",
      "features.9.conv_list.2.bn.num_batches_tracked\n",
      "features.9.conv_list.3.conv.weight\n",
      "features.9.conv_list.3.bn.weight\n",
      "features.9.conv_list.3.bn.bias\n",
      "features.9.conv_list.3.bn.running_mean\n",
      "features.9.conv_list.3.bn.running_var\n",
      "features.9.conv_list.3.bn.num_batches_tracked\n",
      "features.10.conv_list.0.conv.weight\n",
      "features.10.conv_list.0.bn.weight\n",
      "features.10.conv_list.0.bn.bias\n",
      "features.10.conv_list.0.bn.running_mean\n",
      "features.10.conv_list.0.bn.running_var\n",
      "features.10.conv_list.0.bn.num_batches_tracked\n",
      "features.10.conv_list.1.conv.weight\n",
      "features.10.conv_list.1.bn.weight\n",
      "features.10.conv_list.1.bn.bias\n",
      "features.10.conv_list.1.bn.running_mean\n",
      "features.10.conv_list.1.bn.running_var\n",
      "features.10.conv_list.1.bn.num_batches_tracked\n",
      "features.10.conv_list.2.conv.weight\n",
      "features.10.conv_list.2.bn.weight\n",
      "features.10.conv_list.2.bn.bias\n",
      "features.10.conv_list.2.bn.running_mean\n",
      "features.10.conv_list.2.bn.running_var\n",
      "features.10.conv_list.2.bn.num_batches_tracked\n",
      "features.10.conv_list.3.conv.weight\n",
      "features.10.conv_list.3.bn.weight\n",
      "features.10.conv_list.3.bn.bias\n",
      "features.10.conv_list.3.bn.running_mean\n",
      "features.10.conv_list.3.bn.running_var\n",
      "features.10.conv_list.3.bn.num_batches_tracked\n",
      "features.11.conv_list.0.conv.weight\n",
      "features.11.conv_list.0.bn.weight\n",
      "features.11.conv_list.0.bn.bias\n",
      "features.11.conv_list.0.bn.running_mean\n",
      "features.11.conv_list.0.bn.running_var\n",
      "features.11.conv_list.0.bn.num_batches_tracked\n",
      "features.11.conv_list.1.conv.weight\n",
      "features.11.conv_list.1.bn.weight\n",
      "features.11.conv_list.1.bn.bias\n",
      "features.11.conv_list.1.bn.running_mean\n",
      "features.11.conv_list.1.bn.running_var\n",
      "features.11.conv_list.1.bn.num_batches_tracked\n",
      "features.11.conv_list.2.conv.weight\n",
      "features.11.conv_list.2.bn.weight\n",
      "features.11.conv_list.2.bn.bias\n",
      "features.11.conv_list.2.bn.running_mean\n",
      "features.11.conv_list.2.bn.running_var\n",
      "features.11.conv_list.2.bn.num_batches_tracked\n",
      "features.11.conv_list.3.conv.weight\n",
      "features.11.conv_list.3.bn.weight\n",
      "features.11.conv_list.3.bn.bias\n",
      "features.11.conv_list.3.bn.running_mean\n",
      "features.11.conv_list.3.bn.running_var\n",
      "features.11.conv_list.3.bn.num_batches_tracked\n",
      "features.11.avd_layer.0.weight\n",
      "features.11.avd_layer.1.weight\n",
      "features.11.avd_layer.1.bias\n",
      "features.11.avd_layer.1.running_mean\n",
      "features.11.avd_layer.1.running_var\n",
      "features.11.avd_layer.1.num_batches_tracked\n",
      "features.12.conv_list.0.conv.weight\n",
      "features.12.conv_list.0.bn.weight\n",
      "features.12.conv_list.0.bn.bias\n",
      "features.12.conv_list.0.bn.running_mean\n",
      "features.12.conv_list.0.bn.running_var\n",
      "features.12.conv_list.0.bn.num_batches_tracked\n",
      "features.12.conv_list.1.conv.weight\n",
      "features.12.conv_list.1.bn.weight\n",
      "features.12.conv_list.1.bn.bias\n",
      "features.12.conv_list.1.bn.running_mean\n",
      "features.12.conv_list.1.bn.running_var\n",
      "features.12.conv_list.1.bn.num_batches_tracked\n",
      "features.12.conv_list.2.conv.weight\n",
      "features.12.conv_list.2.bn.weight\n",
      "features.12.conv_list.2.bn.bias\n",
      "features.12.conv_list.2.bn.running_mean\n",
      "features.12.conv_list.2.bn.running_var\n",
      "features.12.conv_list.2.bn.num_batches_tracked\n",
      "features.12.conv_list.3.conv.weight\n",
      "features.12.conv_list.3.bn.weight\n",
      "features.12.conv_list.3.bn.bias\n",
      "features.12.conv_list.3.bn.running_mean\n",
      "features.12.conv_list.3.bn.running_var\n",
      "features.12.conv_list.3.bn.num_batches_tracked\n",
      "features.13.conv_list.0.conv.weight\n",
      "features.13.conv_list.0.bn.weight\n",
      "features.13.conv_list.0.bn.bias\n",
      "features.13.conv_list.0.bn.running_mean\n",
      "features.13.conv_list.0.bn.running_var\n",
      "features.13.conv_list.0.bn.num_batches_tracked\n",
      "features.13.conv_list.1.conv.weight\n",
      "features.13.conv_list.1.bn.weight\n",
      "features.13.conv_list.1.bn.bias\n",
      "features.13.conv_list.1.bn.running_mean\n",
      "features.13.conv_list.1.bn.running_var\n",
      "features.13.conv_list.1.bn.num_batches_tracked\n",
      "features.13.conv_list.2.conv.weight\n",
      "features.13.conv_list.2.bn.weight\n",
      "features.13.conv_list.2.bn.bias\n",
      "features.13.conv_list.2.bn.running_mean\n",
      "features.13.conv_list.2.bn.running_var\n",
      "features.13.conv_list.2.bn.num_batches_tracked\n",
      "features.13.conv_list.3.conv.weight\n",
      "features.13.conv_list.3.bn.weight\n",
      "features.13.conv_list.3.bn.bias\n",
      "features.13.conv_list.3.bn.running_mean\n",
      "features.13.conv_list.3.bn.running_var\n",
      "features.13.conv_list.3.bn.num_batches_tracked\n",
      "conv_last.conv.weight\n",
      "conv_last.bn.weight\n",
      "conv_last.bn.bias\n",
      "conv_last.bn.running_mean\n",
      "conv_last.bn.running_var\n",
      "conv_last.bn.num_batches_tracked\n",
      "fc.weight\n",
      "bn.weight\n",
      "bn.bias\n",
      "bn.running_mean\n",
      "bn.running_var\n",
      "bn.num_batches_tracked\n",
      "linear.weight\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"/xiaoou/STDC-Seg-master/STDC-Seg-master/STDC-Seg-weight/STDCNet1446_76.47.tar\",map_location='cpu')[\"state_dict\"]\n",
    "model = BiSeNet('STDCNet1446', 19)\n",
    "for k,v in state_dict.items():\n",
    "    print(k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDCNet1446.pth\t\t\t      __init__.py\n",
      "STDCNet1446_modify.pth\t\t      __pycache__\n",
      "STDCNet1446_modify_ca.pth\t      model_stages.py\n",
      "STDCNet1446_modify_conv.pth\t      model_stages_trt.py\n",
      "STDCNet1446_modify_conv1layeradd.pth  modeltest.ipynb\n",
      "STDCNet1446_modify_convadd.pth\t      stdcnet.py\n",
      "STDCNet1446_modify_convlayeradd.pth\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-651dda21e535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_stages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBiSeNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# models = BiSeNet()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBiSeNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/xiaoou/STDC-Seg-master/STDC-Seg-master/models/model_stages.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdcnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTDCNet1446\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTDCNet813\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# from modules.bn import InPlaceABNSync as BatchNorm2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mBatchNorm2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nets'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from model_stages import BiSeNet\n",
    "# models = BiSeNet()\n",
    "summary(BiSeNet, input_size=(3, 768, 1536))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchsummary\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 384, 768])\n",
      "torch.Size([1, 64, 192, 384])\n",
      "torch.Size([1, 256, 96, 192])\n",
      "torch.Size([1, 512, 48, 96])\n",
      "torch.Size([1, 1024, 24, 48])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import math\n",
    "from resnet_block import Bottleneck\n",
    "BatchNorm2d = nn.BatchNorm2d\n",
    "\n",
    "# class depthwise_separable_conv(nn.Module):\n",
    "#     def init(self, nin, nout):\n",
    "#         super(depthwise_separable_conv, self).init()\n",
    "#         self.depthwise = nn.Conv2d(nin, nin, kernel_size=3, padding=1, groups=nin)\n",
    "#         self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "#     def forward(self, x):\n",
    "#         out = self.depthwise(x)\n",
    "#         out = self.pointwise(out)\n",
    "#         return out\n",
    "# class ConvBNReLU(nn.Module):\n",
    "#     def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n",
    "#         super(ConvBNReLU, self).__init__()\n",
    "#         self.conv = nn.Conv2d(in_chan,\n",
    "#                 out_chan,\n",
    "#                 kernel_size = ks,\n",
    "#                 stride = stride,\n",
    "#                 padding = padding,\n",
    "#                 bias = False)\n",
    "#         # self.bn = BatchNorm2d(out_chan)\n",
    "#         self.bn = BatchNorm2d(out_chan)\n",
    "#         self.relu = nn.ReLU()\n",
    "# #         self.init_weight()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv(x)\n",
    "#         x = self.bn(x)\n",
    "#         x = self.relu(x)\n",
    "#         return x\n",
    "class ConvX(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel=3, stride=1):\n",
    "        super(ConvX, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel, stride=stride, padding=kernel//2, bias=False)\n",
    "#         self.depthwise = nn.Conv2d(in_planes, in_planes, kernel_size=3, padding=1, groups=in_planes)\n",
    "#         self.pointwise = nn.Conv2d(in_planes, out_planes, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.bn(self.conv(x)))\n",
    "#         out = self.relu(self.bn(self.pointwise(self.depthwise(x))))\n",
    "        return out\n",
    "\n",
    "\n",
    "class AddBottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
    "        super(AddBottleneck, self).__init__()\n",
    "        assert block_num > 1, print(\"block number should be larger than 1.\")\n",
    "        self.conv_list = nn.ModuleList()\n",
    "        self.stride = stride\n",
    "        if stride == 2:\n",
    "            self.avd_layer = nn.Sequential(\n",
    "                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n",
    "                nn.BatchNorm2d(out_planes//2),\n",
    "            )\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=2, padding=1, groups=in_planes, bias=False),\n",
    "                nn.BatchNorm2d(in_planes),\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_planes),\n",
    "            )\n",
    "            stride = 1\n",
    "\n",
    "        for idx in range(block_num):\n",
    "            if idx == 0:\n",
    "                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))\n",
    "            elif idx == 1 and block_num == 2:\n",
    "                self.conv_list.append(ConvX(out_planes//2, out_planes//2, stride=stride))\n",
    "            elif idx == 1 and block_num > 2:\n",
    "                self.conv_list.append(ConvX(out_planes//2, out_planes//4, stride=stride))\n",
    "            elif idx < block_num - 1:\n",
    "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1))))\n",
    "            else:\n",
    "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx))))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out_list = []\n",
    "        out = x\n",
    "\n",
    "        for idx, conv in enumerate(self.conv_list):\n",
    "            if idx == 0 and self.stride == 2:\n",
    "                out = self.avd_layer(conv(out))\n",
    "            else:\n",
    "                out = conv(out)\n",
    "            out_list.append(out)\n",
    "\n",
    "        if self.stride == 2:\n",
    "            x = self.skip(x)\n",
    "\n",
    "        return torch.cat(out_list, dim=1) + x\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "class channal_shuffle(nn.Module):\n",
    "    def __init__(self,groups=4):\n",
    "        super(channal_shuffle, self).__init__()\n",
    "        self.groups = groups\n",
    "    def forward(self, x):\n",
    "        batchsize, num_channels, height, width = x.data.size()\n",
    "        channels_per_group = num_channels // self.groups\n",
    "        # grouping, 通道分组\n",
    "        # b, num_channels, h, w =======>  b, groups, channels_per_group, h, w\n",
    "        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n",
    "        # channel shuffle, 通道洗牌\n",
    "        x = torch.transpose(x, 1, 2).contiguous()\n",
    "        # x.shape=(batchsize, channels_per_group, groups, height, width)\n",
    "        # flatten\n",
    "        x = x.view(batchsize, -1, height, width)\n",
    "        return x\n",
    "class CatBottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, block_num=3, stride=1):\n",
    "        super(CatBottleneck, self).__init__()\n",
    "        assert block_num > 1, print(\"block number should be larger than 1.\")\n",
    "        self.conv_list = nn.ModuleList()\n",
    "        self.stride = stride\n",
    "#         self.atenchannal1 = ChannelAttention()\n",
    "#         self.atenchannal2 = SpatialAttention(1024)\n",
    "        self.channal_shuffle = channal_shuffle(4)\n",
    "        if stride == 2:\n",
    "            self.avd_layer = nn.Sequential(\n",
    "                nn.Conv2d(out_planes//2, out_planes//2, kernel_size=3, stride=2, padding=1, groups=out_planes//2, bias=False),\n",
    "                nn.BatchNorm2d(out_planes//2),\n",
    "            )\n",
    "            self.skip = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            stride = 1\n",
    "        self.avg = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "#         self.conavg_layer = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0,bias=False)\n",
    "#         self.conavg_layer_BN = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "#         self.conavg_layer = nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0,bias=False),\n",
    "#                                           nn.BatchNorm2d(out_planes))\n",
    "\n",
    "        self.conavg_layer = nn.Sequential(nn.Conv2d(in_planes, out_planes//4, kernel_size=1, stride=1, padding=0,bias=False),\n",
    "                                          nn.BatchNorm2d(out_planes//4),\n",
    "#                                           nn.ReLU(),\n",
    "                                          nn.Conv2d(out_planes//4, out_planes//4, kernel_size=3, stride=1, padding=1,bias=False),\n",
    "                                         nn.BatchNorm2d(out_planes//4),\n",
    "#                                           nn.ReLU(),\n",
    "                                         nn.Conv2d(out_planes//4, out_planes, kernel_size=1, stride=1, padding=0,bias=False),\n",
    "                                         nn.BatchNorm2d(out_planes),nn.Sigmoid())\n",
    "    \n",
    "#         self.channal_link = nn.Sequential(nn.Conv2d(out_planes, out_planes, kernel_size=1, stride=1, padding=0,bias=False),\n",
    "#                                           nn.BatchNorm2d(out_planes))\n",
    "        \n",
    "\n",
    "        for idx in range(block_num):\n",
    "            if idx == 0:\n",
    "                self.conv_list.append(ConvX(in_planes, out_planes//2, kernel=1))\n",
    "#                 self.conv_list.append()\n",
    "#                 self.conv_list.append(ChannelAttention(out_planes//2))\n",
    "            elif idx == 1 and block_num == 2:\n",
    "                self.conv_list.append(ConvX(out_planes//2, out_planes//2, stride=stride))\n",
    "\n",
    "#                 self.conv_list.append(ChannelAttention(out_planes//2))\n",
    "            elif idx == 1 and block_num > 2:\n",
    "                self.conv_list.append(ConvX(out_planes//2, out_planes//4, stride=stride))\n",
    "\n",
    "#                 self.conv_list.append(ChannelAttention(out_planes//4))\n",
    "            elif idx < block_num - 1:\n",
    "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx+1))))\n",
    "\n",
    "#                 self.conv_list.append(ChannelAttention(out_planes//int(math.pow(2, idx+1))))\n",
    "            else:\n",
    "                self.conv_list.append(ConvX(out_planes//int(math.pow(2, idx)), out_planes//int(math.pow(2, idx))))\n",
    "              \n",
    "#                 self.conv_list.append(ChannelAttention(out_planes//int(math.pow(2, idx))))\n",
    "                                      \n",
    "    def forward(self, x):\n",
    "        out_list = []\n",
    "        out1 = self.conv_list[0](x)\n",
    "#         out_atten = self.atenchannal2(out1)\n",
    "        if self.stride==2:\n",
    "            x = self.avg(x)\n",
    "#         out_temp = self.atenchannal2(self.conavg_layer(x))\n",
    "        out_temp = self.conavg_layer(x)\n",
    "#         out_temp = self.atenchannal2(out_temp)*out_temp\n",
    "#         out_temp = self.conavg_layer_BN(out_temp)\n",
    "        \n",
    "        \n",
    "#         print(\"out_temp:\")\n",
    "#         print(out_temp.size())\n",
    "\n",
    "        for idx, conv in enumerate(self.conv_list[1:]):\n",
    "            if idx == 0:\n",
    "                if self.stride == 2:\n",
    "                    out = conv(self.avd_layer(out1))\n",
    "#                     print(\"out2:\")\n",
    "#                     print(out.size())\n",
    "#                     out = self.conv_list[2*idx+3](out1)\n",
    "#                     print(\"out2:\")\n",
    "#                     print(out.size())\n",
    "                else:\n",
    "                    out = conv(out1)\n",
    "#                     out = self.conv_list[2*idx+3](out1)+out\n",
    "#                     print(\"out2:\")\n",
    "#                     print(out.size())\n",
    "            else:\n",
    "                out = conv(out)\n",
    "#                 out = atten + out\n",
    "#                 out = (self.conv_list[2*idx+3](out))+out_temp\n",
    "#                 print(\"out:\")\n",
    "#                 print(out.size())\n",
    "            out_list.append(out)\n",
    "\n",
    "        if self.stride == 2:\n",
    "            out1 = self.skip(out1)\n",
    "        out_list.insert(0, out1)\n",
    "\n",
    "        out = torch.cat(out_list, dim=1)\n",
    "#         print(\"output:\")\n",
    "#         print(out.size())\n",
    "#         out = self.channal_link(out)\n",
    "        out = out+out_temp\n",
    "#         out = self.channal_shuffle(out)\n",
    "#         out = self.atenchannal2(out)*out\n",
    "#         out = outlist[]\n",
    "        return out\n",
    "\n",
    "#STDC2Net\n",
    "class STDCNet1446(nn.Module):\n",
    "    def __init__(self, block_res,layers_res,base=64, layers=[2,2,2], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20, pretrain_model='', use_conv_last=False):\n",
    "        super(STDCNet1446, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1,\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block_res, 64, layers_res[0],stride=2)\n",
    "        self.layer2 = self._make_layer(block_res, 256, layers_res[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block_res, 512, layers_res[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block_res, 1024, layers_res[3], stride=2)\n",
    "#         if type == \"cat\":\n",
    "#             block = CatBottleneck\n",
    "#         elif type == \"add\":\n",
    "#             block = AddBottleneck\n",
    "        self.use_conv_last = use_conv_last\n",
    "#         self.features = self._make_layers(base, layers, block_num, block)\n",
    "#         self.conv_last = ConvX(base*16, max(1024, base*16), 1, 1)\n",
    "#         self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.fc = nn.Linear(max(1024, base*16), max(1024, base*16), bias=False)\n",
    "#         self.bn = nn.BatchNorm1d(max(1024, base*16))\n",
    "#         self.relu = nn.ReLU(inplace=True)\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         self.linear = nn.Linear(max(1024, base*16), num_classes, bias=False)\n",
    "\n",
    "#         self.atenchannal1 = ChannelAttention(1024)\n",
    "#         self.atenchannal2 = SpatialAttention()\n",
    "#         self.atenchannal2_3 = ChannelAttention(256)\n",
    "#         self.atenchannal2_4 = ChannelAttention(512)\n",
    "#         self.atenchannal2_5 = ChannelAttention(1024)\n",
    "#         self.spatten = SpatialAttention()\n",
    "#         self.trans4 = ConvX(base,base*2,3,2)\n",
    "#         self.trans8 = ConvX(base*2,base*4,3,2)\n",
    "#         self.trans16 = ConvX(base*4,base*8,3,2)\n",
    "#         self.concatfuse8 = ConvX(base*4, base*4)\n",
    "#         self.concatfuse16 = ConvX(base*8, base*8)\n",
    "#         self.concatfuse32 = ConvX(base*16, base*16)\n",
    "\n",
    "#         self.x2 = nn.Sequential(self.features[:1])\n",
    "#         self.x4 = nn.Sequential(self.features[1:2])\n",
    "#         self.x8 = nn.Sequential(self.features[2:6])\n",
    "#         self.t256 = nn.Sequential(self.features[6])\n",
    "#         self.addlayer7 = nn.Sequential(self.features[7])\n",
    "#         self.x16 = nn.Sequential(self.features[8:13])\n",
    "#         self.t512 = nn.Sequential(self.features[13])\n",
    "#         self.addlayer14 = nn.Sequential(self.features[14])\n",
    "#         self.x32 = nn.Sequential(self.features[15:18])\n",
    "#         self.t1024 = nn.Sequential(self.features[18])\n",
    "#         self.addlayer19 = nn.Sequential(self.features[19])\n",
    "\n",
    "#         self.x2 = nn.Sequential(self.features[:1])\n",
    "#         self.x4 = nn.Sequential(self.features[1:2])\n",
    "#         self.x8 = nn.Sequential(self.features[2:7])\n",
    "#         self.t256 = nn.Sequential(self.features[7])\n",
    "#         self.addlayer7 = nn.Sequential(self.features[8])\n",
    "#         self.x16 = nn.Sequential(self.features[9:15])\n",
    "#         self.t512 = nn.Sequential(self.features[15])\n",
    "#         self.addlayer14 = nn.Sequential(self.features[16])\n",
    "#         self.x32 = nn.Sequential(self.features[17:21])\n",
    "#         self.t1024 = nn.Sequential(self.features[21])\n",
    "#         self.addlayer19 = nn.Sequential(self.features[22])\n",
    "\n",
    "#         self.x2 = nn.Sequential(self.features[:1])\n",
    "#         self.x4 = nn.Sequential(self.features[1:2])\n",
    "#         self.x8 = nn.Sequential(self.features[2:6])\n",
    "#         self.x16 = nn.Sequential(self.features[6:11])\n",
    "#         self.x32 = nn.Sequential(self.features[11:])\n",
    "\n",
    "#         self.x2 = nn.Sequential(self.features[:1])\n",
    "#         self.x4 = nn.Sequential(self.features[1:2])\n",
    "#         self.x8 = nn.Sequential(self.features[2:5])\n",
    "#         self.x16 = nn.Sequential(self.features[5:9])\n",
    "#         self.x32 = nn.Sequential(self.features[9:])\n",
    "\n",
    "#         self.x2 = nn.Sequential(self.features[:1])\n",
    "#         self.x4 = nn.Sequential(self.features[1:2])\n",
    "#         self.x8 = nn.Sequential(self.features[2:4])\n",
    "#         self.x16 = nn.Sequential(self.features[4:6])\n",
    "#         self.x32 = nn.Sequential(self.features[6:])\n",
    "        \n",
    "#         self.x2 = nn.Sequential(self.features[:1])\n",
    "#         self.x4 = nn.Sequential(self.features[1:2])\n",
    "#         self.x8 = nn.Sequential(self.features[2:7])\n",
    "#         self.x16 = nn.Sequential(self.features[7:13])\n",
    "#         self.x32 = nn.Sequential(self.features[13:])\n",
    "        \n",
    "\n",
    "        if pretrain_model:\n",
    "#             print('use pretrain model {}'.format(pretrain_model))\n",
    "#             self.init_weight(pretrain_model)\n",
    "#         else:\n",
    "            self.init_params()\n",
    "\n",
    "    def init_weight(self, pretrain_model):\n",
    "        \n",
    "        state_dict = torch.load(pretrain_model)\n",
    "        self_state_dict = self.state_dict()\n",
    "        sub_list = list(state_dict.keys())[:295]\n",
    "        for k, v in state_dict.items():\n",
    "            if k in sub_list:\n",
    "                k = k.replace('cp.backbone.','')\n",
    "                self_state_dict.update({k: v})\n",
    "        self.load_state_dict(self_state_dict)\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "#     def _make_layers(self, base, layers, block_num, block):\n",
    "#         features = []\n",
    "#         features += [ConvX(3, base//2, 3, 2)]\n",
    "#         features += [ConvX(base//2, base, 3, 2)]\n",
    "\n",
    "#         for i, layer in enumerate(layers):\n",
    "#             for j in range(layer):\n",
    "#                 if i == 0 and j == 0:\n",
    "#                     features.append(block(base, base*4, block_num, 2))\n",
    "#                 elif j == 0:\n",
    "#                     features.append(block(base*int(math.pow(2,i+1)), base*int(math.pow(2,i+2)), block_num, 2))\n",
    "# #                 elif j+2==layer:\n",
    "# #                     features.append(block(base*int(math.pow(2,i+1)),base*int(math.pow(2,i+2))))\n",
    "# #                 elif j+2>layer:\n",
    "# #                     features.append(block(base*int(math.pow(2,i+2)),base*int(math.pow(2,i+2))))\n",
    "#                 else:\n",
    "#                     features.append(block(base*int(math.pow(2,i+2)), base*int(math.pow(2,i+2)), block_num, 1))\n",
    "# #             features.append(block(base*int(math.pow(2,i+1)),base*int(math.pow(2,i+2))))\n",
    "\n",
    "#         return nn.Sequential(*features)\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    " \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    " \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "        \n",
    "#         feat2 = self.x2(x)\n",
    "        feat2 = x\n",
    "        print(feat2.size())\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "#         feat4 = self.x4(feat2)\n",
    "        feat4 = self.layer1(x)\n",
    "        print(feat4.size())\n",
    "#         print(\"feat4:\")\n",
    "#         print(feat4.size())\n",
    "        \n",
    "#         trans4 = self.trans4(feat4)\n",
    "        \n",
    "#         feat8_temp = self.x8(feat4)\n",
    "#         feat8tmp = self.x8(feat4)\n",
    "#         feat8 = self.x8(feat4)\n",
    "        feat8 = self.layer2(feat4)\n",
    "        print(feat8.size())\n",
    "#         feat8 = self.addlayer7(self.t256(feat8tmp))\n",
    "#         feat8_ori = self.addlayer7(self.t256(feat8tmp))\n",
    "#         feat8 = self.atenchannal2_3(feat8_ori)*feat8_ori\n",
    "#         feat8 = self.spatten(feat8)*feat8\n",
    "#         feat8 = feat8+feat8_ori\n",
    "#         feat8 = self.t256(feat8tmp)\n",
    "#         feat8 = torch.cat((trans4,feat8_temp),dim=1)\n",
    "#         feat8 = self.concatfuse8(feat8)\n",
    "#         print(\"feat8:\")\n",
    "#         print(feat8.size())\n",
    "        \n",
    "#         trans8 = self.trans8(feat8_temp)\n",
    "        \n",
    "#         feat16tmp = self.x16(feat8tmp)\n",
    "#         feat16 = self.x16(feat8)\n",
    "        feat16 = self.layer3(feat8)\n",
    "        print(feat16.size())\n",
    "#         print(\"feat16tmp:\")\n",
    "#         print(feat16tmp.size())\n",
    "#         feat16 = self.addlayer14(self.t512(feat16tmp))\n",
    "#         feat16_ori = self.addlayer14(self.t512(feat16tmp))\n",
    "#         feat16 = self.atenchannal2_4(feat16_ori)*feat16_ori\n",
    "#         feat16 = self.spatten(feat16)*feat16\n",
    "#         feat16 = feat16+feat16_ori\n",
    "#         feat16 = self.t512(feat16tmp)\n",
    "#         feat16_temp = self.x16(feat8_temp)\n",
    "#         feat16 = torch.cat((trans8,feat16_temp),dim=1)\n",
    "#         feat16 = self.concatfuse16(feat16)\n",
    "#         print(\"feat16:\")\n",
    "#         print(feat16.size())\n",
    "        \n",
    "#         trans16 = self.trans16(feat16_temp)\n",
    "        \n",
    "#         feat32tmp = self.x32(feat16tmp)\n",
    "#         feat32 = self.x32(feat16)\n",
    "        feat32 = self.layer4(feat16)\n",
    "        print(feat32.size())\n",
    "#         feat32 = self.atenchannal1(feat32)*feat32\n",
    "#         feat32 = self.atenchannal2(feat32)*feat32\n",
    "#         feat32 = self.addlayer19(self.t1024(feat32tmp))\n",
    "#         feat32_ori = self.addlayer19(self.t1024(feat32tmp))\n",
    "#         feat32 = self.atenchannal2_5(feat32_ori)*feat32_ori\n",
    "#         feat32 = self.spatten(feat32)*feat32\n",
    "#         feat32 = feat32+feat32_ori\n",
    "#         feat32 = self.t1024(feat32tmp)\n",
    "#         feat32 = torch.cat((trans16,feat32_temp),dim=1)\n",
    "#         feat32 = self.concatfuse32(feat32)\n",
    "#         print(\"feat32:\")\n",
    "#         print(feat32.size())\n",
    "        \n",
    "        if self.use_conv_last:\n",
    "           feat32 = self.conv_last(feat32)\n",
    "\n",
    "        return feat2, feat4, feat8, feat16, feat32\n",
    "\n",
    "    def forward_impl(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.conv_last(out).pow(2)\n",
    "        out = self.gap(out).flatten(1)\n",
    "        out = self.fc(out)\n",
    "        # out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        # out = self.relu(self.bn(self.fc(out)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# STDC1Net\n",
    "class STDCNet813(nn.Module):\n",
    "    def __init__(self, base=64, layers=[2,2,2], block_num=4, type=\"cat\", num_classes=1000, dropout=0.20, pretrain_model='', use_conv_last=False):\n",
    "        super(STDCNet813, self).__init__()\n",
    "        if type == \"cat\":\n",
    "            block = CatBottleneck\n",
    "        elif type == \"add\":\n",
    "            block = AddBottleneck\n",
    "        self.use_conv_last = use_conv_last\n",
    "        self.features = self._make_layers(base, layers, block_num, block)\n",
    "        self.conv_last = ConvX(base*16, max(1024, base*16), 1, 1)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(max(1024, base*16), max(1024, base*16), bias=False)\n",
    "        self.bn = nn.BatchNorm1d(max(1024, base*16))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear = nn.Linear(max(1024, base*16), num_classes, bias=False)\n",
    "\n",
    "        self.x2 = nn.Sequential(self.features[:1])\n",
    "        self.x4 = nn.Sequential(self.features[1:2])\n",
    "        self.x8 = nn.Sequential(self.features[2:4])\n",
    "        self.x16 = nn.Sequential(self.features[4:6])\n",
    "        self.x32 = nn.Sequential(self.features[6:])\n",
    "\n",
    "        if pretrain_model:\n",
    "            print('use pretrain model {}'.format(pretrain_model))\n",
    "            self.init_weight(pretrain_model)\n",
    "        else:\n",
    "            self.init_params()\n",
    "\n",
    "    def init_weight(self, pretrain_model):\n",
    "        \n",
    "        state_dict = torch.load(pretrain_model)[\"state_dict\"]\n",
    "        self_state_dict = self.state_dict()\n",
    "        for k, v in state_dict.items():\n",
    "            self_state_dict.update({k: v})\n",
    "        self.load_state_dict(self_state_dict)\n",
    "\n",
    "    def init_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layers(self, base, layers, block_num, block):\n",
    "        features = []\n",
    "        features += [ConvX(3, base//2, 3, 2)]\n",
    "        features += [ConvX(base//2, base, 3, 2)]\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "            for j in range(layer):\n",
    "                if i == 0 and j == 0:\n",
    "                    features.append(block(base, base*4, block_num, 2))\n",
    "                elif j == 0:\n",
    "                    features.append(block(base*int(math.pow(2,i+1)), base*int(math.pow(2,i+2)), block_num, 2))\n",
    "                else:\n",
    "                    features.append(block(base*int(math.pow(2,i+2)), base*int(math.pow(2,i+2)), block_num, 1))\n",
    "\n",
    "        return nn.Sequential(*features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat2 = self.x2(x)\n",
    "        feat4 = self.x4(feat2)\n",
    "        feat8 = self.x8(feat4)\n",
    "        feat16 = self.x16(feat8)\n",
    "        feat32 = self.x32(feat16)\n",
    "        if self.use_conv_last:\n",
    "           feat32 = self.conv_last(feat32)\n",
    "\n",
    "        return feat2, feat4, feat8, feat16, feat32\n",
    "\n",
    "    def forward_impl(self, x):\n",
    "        out = self.features(x)\n",
    "        out = self.conv_last(out).pow(2)\n",
    "        out = self.gap(out).flatten(1)\n",
    "        out = self.fc(out)\n",
    "        # out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        # out = self.relu(self.bn(self.fc(out)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_b = STDCNet1446(Bottleneck,[3,4,6,3],num_classes=1000, dropout=0.00, block_num=4)\n",
    "    model_b.eval()\n",
    "    x = torch.randn(1,3,768,1536)\n",
    "    y = model(x)\n",
    "    torch.save(model_b.state_dict(), 'cat_res.pth')\n",
    "#     print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: module Bottleneck is treated as a zero-op.\n",
      "Warning: module STDCNet1446 is treated as a zero-op.\n",
      "torch.Size([1, 32, 384, 768])\n",
      "torch.Size([1, 64, 192, 384])\n",
      "torch.Size([1, 256, 96, 192])\n",
      "torch.Size([1, 512, 48, 96])\n",
      "torch.Size([1, 1024, 24, 48])\n",
      "STDCNet1446(\n",
      "  104.039 M, 100.000% Params, 421.594 GMac, 100.000% MACs, \n",
      "  (conv1): Conv2d(0.005 M, 0.005% Params, 1.387 GMac, 0.329% MACs, 3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(0.0 M, 0.000% Params, 0.019 GMac, 0.004% MACs, 32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(0.0 M, 0.000% Params, 0.028 GMac, 0.007% MACs, inplace=True)\n",
      "  (conv2): Conv2d(0.018 M, 0.018% Params, 5.436 GMac, 1.289% MACs, 32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn2): BatchNorm2d(0.0 M, 0.000% Params, 0.038 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    0.141 M, 0.135% Params, 11.353 GMac, 2.693% MACs, \n",
      "    (0): Bottleneck(\n",
      "      0.05 M, 0.048% Params, 4.624 GMac, 1.097% MACs, \n",
      "      (conv1): Conv2d(0.004 M, 0.004% Params, 1.208 GMac, 0.287% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.000% Params, 0.038 GMac, 0.009% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.037 M, 0.035% Params, 2.718 GMac, 0.645% MACs, 64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.004 M, 0.004% Params, 0.302 GMac, 0.072% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.028 GMac, 0.007% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        0.004 M, 0.004% Params, 0.311 GMac, 0.074% MACs, \n",
      "        (0): Conv2d(0.004 M, 0.004% Params, 0.302 GMac, 0.072% MACs, 64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      0.045 M, 0.044% Params, 3.364 GMac, 0.798% MACs, \n",
      "      (conv1): Conv2d(0.004 M, 0.004% Params, 0.302 GMac, 0.072% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.037 M, 0.035% Params, 2.718 GMac, 0.645% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.004 M, 0.004% Params, 0.302 GMac, 0.072% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.014 GMac, 0.003% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      0.045 M, 0.044% Params, 3.364 GMac, 0.798% MACs, \n",
      "      (conv1): Conv2d(0.004 M, 0.004% Params, 0.302 GMac, 0.072% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.037 M, 0.035% Params, 2.718 GMac, 0.645% MACs, 64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.004 M, 0.004% Params, 0.302 GMac, 0.072% MACs, 64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.0 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.014 GMac, 0.003% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    2.857 M, 2.747% Params, 53.674 GMac, 12.731% MACs, \n",
      "    (0): Bottleneck(\n",
      "      0.69 M, 0.663% Params, 13.684 GMac, 3.246% MACs, \n",
      "      (conv1): Conv2d(0.016 M, 0.016% Params, 1.208 GMac, 0.287% MACs, 64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.000% Params, 0.038 GMac, 0.009% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.59 M, 0.567% Params, 10.872 GMac, 2.579% MACs, 256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.028 GMac, 0.007% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        0.017 M, 0.016% Params, 0.311 GMac, 0.074% MACs, \n",
      "        (0): Conv2d(0.016 M, 0.016% Params, 0.302 GMac, 0.072% MACs, 64, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      0.722 M, 0.694% Params, 13.33 GMac, 3.162% MACs, \n",
      "      (conv1): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.59 M, 0.567% Params, 10.872 GMac, 2.579% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.014 GMac, 0.003% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      0.722 M, 0.694% Params, 13.33 GMac, 3.162% MACs, \n",
      "      (conv1): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.59 M, 0.567% Params, 10.872 GMac, 2.579% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.014 GMac, 0.003% MACs, inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      0.722 M, 0.694% Params, 13.33 GMac, 3.162% MACs, \n",
      "      (conv1): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(0.59 M, 0.567% Params, 10.872 GMac, 2.579% MACs, 256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.066 M, 0.063% Params, 1.208 GMac, 0.287% MACs, 256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.000% Params, 0.009 GMac, 0.002% MACs, 256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.014 GMac, 0.003% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    66.394 M, 63.817% Params, 307.94 GMac, 73.042% MACs, \n",
      "    (0): Bottleneck(\n",
      "      2.888 M, 2.776% Params, 15.147 GMac, 3.593% MACs, \n",
      "      (conv1): Conv2d(0.131 M, 0.126% Params, 2.416 GMac, 0.573% MACs, 256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.019 GMac, 0.004% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.014 GMac, 0.003% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        0.132 M, 0.127% Params, 0.609 GMac, 0.144% MACs, \n",
      "        (0): Conv2d(0.131 M, 0.126% Params, 0.604 GMac, 0.143% MACs, 256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (6): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (7): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (8): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (9): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (10): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (11): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (12): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (13): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (14): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (15): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (16): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (17): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (18): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (19): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (20): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (21): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "    (22): Bottleneck(\n",
      "      2.887 M, 2.775% Params, 13.309 GMac, 3.157% MACs, \n",
      "      (conv1): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(2.359 M, 2.268% Params, 10.872 GMac, 2.579% MACs, 512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(0.262 M, 0.252% Params, 1.208 GMac, 0.287% MACs, 512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.001 M, 0.001% Params, 0.005 GMac, 0.001% MACs, 512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    34.623 M, 33.279% Params, 41.719 GMac, 9.896% MACs, \n",
      "    (0): Bottleneck(\n",
      "      11.543 M, 11.094% Params, 15.123 GMac, 3.587% MACs, \n",
      "      (conv1): Conv2d(0.524 M, 0.504% Params, 2.416 GMac, 0.573% MACs, 512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.002% Params, 0.009 GMac, 0.002% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(9.437 M, 9.071% Params, 10.872 GMac, 2.579% MACs, 1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.049 M, 1.008% Params, 1.208 GMac, 0.287% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.007 GMac, 0.002% MACs, inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        0.526 M, 0.506% Params, 0.606 GMac, 0.144% MACs, \n",
      "        (0): Conv2d(0.524 M, 0.504% Params, 0.604 GMac, 0.143% MACs, 512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      11.54 M, 11.092% Params, 13.298 GMac, 3.154% MACs, \n",
      "      (conv1): Conv2d(1.049 M, 1.008% Params, 1.208 GMac, 0.287% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(9.437 M, 9.071% Params, 10.872 GMac, 2.579% MACs, 1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.049 M, 1.008% Params, 1.208 GMac, 0.287% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.004 GMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      11.54 M, 11.092% Params, 13.298 GMac, 3.154% MACs, \n",
      "      (conv1): Conv2d(1.049 M, 1.008% Params, 1.208 GMac, 0.287% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(9.437 M, 9.071% Params, 10.872 GMac, 2.579% MACs, 1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(1.049 M, 1.008% Params, 1.208 GMac, 0.287% MACs, 1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(0.002 M, 0.002% Params, 0.002 GMac, 0.001% MACs, 1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(0.0 M, 0.000% Params, 0.004 GMac, 0.001% MACs, inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "from torchvision import models\n",
    "\n",
    "ops, params = get_model_complexity_info(model_b, (3, 768, 1536), as_strings=True, print_per_layer_stat=True, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0e6af709e677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# state_dict = torch.load(\"/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s.pt\")#加载原来的模型  在torch=1.6时加载\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s2.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#不是zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# state_dict_new = torch.load(\"/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s.pt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# lz = torch.load('/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s.pt',_use_new_zipfile_serialization=False)\n",
    "import torch\n",
    "# state_dict = torch.load(\"/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s.pt\")#加载原来的模型  在torch=1.6时加载\n",
    "torch.save(state_dict, \"/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s2.pt\", _use_new_zipfile_serialization=False)\n",
    "#不是zip\n",
    "# state_dict_new = torch.load(\"/xiaoou/STDC-Seg-master/STDC-Seg-master/yolov5s.pt\")\n",
    "# for k, v in state_dict_new.items():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
