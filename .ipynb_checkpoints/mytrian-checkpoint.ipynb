{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- encoding: utf-8 -*-\n",
    "from logger import setup_logger\n",
    "from models.model_stages import BiSeNet\n",
    "from cityscapes import CityScapes\n",
    "from loss.loss import OhemCELoss\n",
    "from loss.detail_loss import DetailAggregateLoss\n",
    "from evaluation import MscEvalV0\n",
    "from optimizer_loss import Optimizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pritn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f21681a48db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpritn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pritn' is not defined"
     ]
    }
   ],
   "source": [
    "pritn(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parse = argparse.ArgumentParser()\n",
    "    parse.add_argument(\n",
    "            '--local_rank',\n",
    "            dest = 'local_rank',\n",
    "            type = int,\n",
    "            default = -1,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--n_workers_train',\n",
    "            dest = 'n_workers_train',\n",
    "            type = int,\n",
    "            default = 12,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--n_workers_val',\n",
    "            dest = 'n_workers_val',\n",
    "            type = int,\n",
    "            default = 1,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--n_img_per_gpu',\n",
    "            dest = 'n_img_per_gpu',\n",
    "            type = int,\n",
    "            default = 16,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--max_iter',\n",
    "            dest = 'max_iter',\n",
    "            type = int,\n",
    "            default = 60000,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--save_iter_sep',\n",
    "            dest = 'save_iter_sep',\n",
    "            type = int,\n",
    "            default = 1000,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--warmup_steps',\n",
    "            dest = 'warmup_steps',\n",
    "            type = int,\n",
    "            default = 1000,\n",
    "            )      \n",
    "    parse.add_argument(\n",
    "            '--mode',\n",
    "            dest = 'mode',\n",
    "            type = str,\n",
    "            default = 'train',\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--ckpt',\n",
    "            dest = 'ckpt',\n",
    "            type = str,\n",
    "            default = None,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--respath',\n",
    "            dest = 'respath',\n",
    "            type = str,\n",
    "            default = '/xiaoou/STDC-Seg-master/STDC-Seg-master/checkpoint/train_STDC_seg/',\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--backbone',\n",
    "            dest = 'backbone',\n",
    "            type = str,\n",
    "            default = 'STDCNet1446',\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--pretrain_path',\n",
    "            dest = 'pretrain_path',\n",
    "            type = str,\n",
    "            default = '/xiaoou/STDC-Seg-master/STDC-Seg-master/STDC-Seg-weight/STDCNet1446_76.47.tar',\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--use_conv_last',\n",
    "            dest = 'use_conv_last',\n",
    "            type = str2bool,\n",
    "            default = False,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--use_boundary_2',\n",
    "            dest = 'use_boundary_2',\n",
    "            type = str2bool,\n",
    "            default = False,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--use_boundary_4',\n",
    "            dest = 'use_boundary_4',\n",
    "            type = str2bool,\n",
    "            default = False,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--use_boundary_8',\n",
    "            dest = 'use_boundary_8',\n",
    "            type = str2bool,\n",
    "            default = True,\n",
    "            )\n",
    "    parse.add_argument(\n",
    "            '--use_boundary_16',\n",
    "            dest = 'use_boundary_16',\n",
    "            type = str2bool,\n",
    "            default = False,\n",
    "            )\n",
    "    args = parse.parse_args(args=[])\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_camvid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import set_cfg_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CityScapes(dspth, cropsize=cropsize, mode=mode, randomscale=randomscale)\n",
    "#     sampler = torch.utils.data.distributed.DistributedSampler(ds)\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = n_img_per_gpu,\n",
    "                shuffle = False,\n",
    "#                     sampler = ds,\n",
    "                num_workers = n_workers_train,\n",
    "                pin_memory = False,\n",
    "                drop_last = True)\n",
    "# exit(0)\n",
    "dsval = CityScapes(dspth, mode='val', randomscale=randomscale)\n",
    "#     sampler_val = torch.utils.data.distributed.DistributedSampler(dsval)\n",
    "dlval = DataLoader(dsval,\n",
    "                batch_size = 2,\n",
    "                shuffle = False,\n",
    "#                     sampler = sampler_val,\n",
    "                num_workers = n_workers_val,\n",
    "                drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<configs.cfg_dict at 0x7fbe97231048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse = argparse.ArgumentParser()\n",
    "parse.add_argument('--config', dest='config', type=str,\n",
    "        default='/xiaoou/STDC-Seg-master/configs/need.py',)\n",
    "parse.add_argument('--finetune-from', type=str, default=None,)\n",
    "\n",
    "args_0 = parse.parse_known_args()[0]\n",
    "\n",
    "args_0\n",
    "\n",
    "cfg = set_cfg_from_file(args_0.config)\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_camvid_loader.get_data_loader(cfg, mode='train')\n",
    "\n",
    "dlval = get_camvid_loader.get_data_loader(cfg, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data in enumerate(dlval):\n",
    "    img,label = data\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_np = label.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(label_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(backbone='STDCNet1446', ckpt=None, local_rank=-1, max_iter=60000, mode='train', n_img_per_gpu=16, n_workers_train=12, n_workers_val=1, pretrain_path='/xiaoou/STDC-Seg-master/STDC-Seg-master/STDC-Seg-weight/STDCNet1446_76.47.tar', respath='/xiaoou/STDC-Seg-master/STDC-Seg-master/checkpoint/train_STDC_seg/', save_iter_sep=1000, use_boundary_16=False, use_boundary_2=False, use_boundary_4=False, use_boundary_8=True, use_conv_last=False, warmup_steps=1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/xiaoou/STDC-Seg-master/STDC-Seg-master/models/model_stages.py:129: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.W.weight, 0)\n",
      "/xiaoou/STDC-Seg-master/STDC-Seg-master/models/model_stages.py:130: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.W.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "ignore_idx = 255\n",
    "net = BiSeNet(backbone=args.backbone, n_classes=11, pretrain_model=args.pretrain_path, \n",
    "use_boundary_2=args.use_boundary_2, use_boundary_4=args.use_boundary_4, use_boundary_8=args.use_boundary_8, \n",
    "use_boundary_16=args.use_boundary_16, use_conv_last=args.use_conv_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2,3,720,960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outs)):\n",
    "    print(outs[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(backbone='STDCNet1446', ckpt=None, local_rank=-1, max_iter=60000, mode='train', n_img_per_gpu=16, n_workers_train=12, n_workers_val=1, pretrain_path='/xiaoou/STDC-Seg-master/STDC-Seg-master/STDC-Seg-weight/STDCNet1446_76.47.tar', respath='/xiaoou/STDC-Seg-master/STDC-Seg-master/checkpoint/train_STDC_seg/', save_iter_sep=1000, use_boundary_16=False, use_boundary_2=False, use_boundary_4=False, use_boundary_8=True, use_conv_last=False, warmup_steps=1000)\n",
      "out torch.Size([8, 19, 720, 960])\n",
      "lb torch.Size([8, 720, 960])\n",
      "out torch.float32\n",
      "lb torch.float32\n",
      "out torch.Size([8, 19, 720, 960])\n",
      "lb torch.Size([8, 720, 960])\n",
      "out torch.float32\n",
      "lb torch.float32\n",
      "out torch.Size([8, 19, 720, 960])\n",
      "lb torch.Size([8, 720, 960])\n",
      "out torch.float32\n",
      "lb torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _MultiProcessingDataLoaderIter.__del__ of <torch.utils.data.dataloader._MultiProcessingDataLoaderIter object at 0x7fbe97029828>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1328, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 1301, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 124, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 47, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6d4f3dc8dbbb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mditer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mn_img_per_gpu\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    171\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6d4f3dc8dbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-6d4f3dc8dbbb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m#             sampler.set_epoch(epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mditer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mditer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    args = parse_args()\n",
    "    print(args)\n",
    "    save_pth_path = os.path.join(args.respath, 'pths')\n",
    "    dspth = '/xiaoou/my_last_project/dataset/cityscapes'\n",
    "    \n",
    "    # print(save_pth_path)\n",
    "    # print(osp.exists(save_pth_path))\n",
    "    # if not osp.exists(save_pth_path) and dist.get_rank()==0: \n",
    "    if not osp.exists(save_pth_path):\n",
    "        os.makedirs(save_pth_path)\n",
    "    \n",
    "    \n",
    "#     torch.cuda.set_device(args.local_rank)\n",
    "#     dist.init_process_group(\n",
    "#                 backend = 'nccl',\n",
    "#                 init_method = 'tcp://127.0.0.1:33274',\n",
    "#                 world_size = torch.cuda.device_count(),\n",
    "#                 rank=args.local_rank\n",
    "#                 )\n",
    "    \n",
    "    setup_logger(args.respath)\n",
    "    ## dataset\n",
    "    n_classes = 19\n",
    "    n_img_per_gpu = args.n_img_per_gpu\n",
    "    n_workers_train = args.n_workers_train\n",
    "    n_workers_val = args.n_workers_val\n",
    "    use_boundary_16 = args.use_boundary_16\n",
    "    use_boundary_8 = args.use_boundary_8\n",
    "    use_boundary_4 = args.use_boundary_4\n",
    "    use_boundary_2 = args.use_boundary_2\n",
    "    \n",
    "    mode = args.mode\n",
    "    cropsize = [720,960]\n",
    "    randomscale = (0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0, 1.125, 1.25, 1.375, 1.5)\n",
    "\n",
    "#     if dist.get_rank()==0: \n",
    "#         logger.info('n_workers_train: {}'.format(n_workers_train))\n",
    "#         logger.info('n_workers_val: {}'.format(n_workers_val))\n",
    "#         logger.info('use_boundary_2: {}'.format(use_boundary_2))\n",
    "#         logger.info('use_boundary_4: {}'.format(use_boundary_4))\n",
    "#         logger.info('use_boundary_8: {}'.format(use_boundary_8))\n",
    "#         logger.info('use_boundary_16: {}'.format(use_boundary_16))\n",
    "#         logger.info('mode: {}'.format(args.mode))\n",
    "    \n",
    "    \n",
    "#     ds = CityScapes(dspth, cropsize=cropsize, mode=mode, randomscale=randomscale)\n",
    "# #     sampler = torch.utils.data.distributed.DistributedSampler(ds)\n",
    "#     dl = DataLoader(ds,\n",
    "#                     batch_size = n_img_per_gpu,\n",
    "#                     shuffle = False,\n",
    "# #                     sampler = ds,\n",
    "#                     num_workers = n_workers_train,\n",
    "#                     pin_memory = False,\n",
    "#                     drop_last = True)\n",
    "#     # exit(0)\n",
    "#     dsval = CityScapes(dspth, mode='val', randomscale=randomscale)\n",
    "# #     sampler_val = torch.utils.data.distributed.DistributedSampler(dsval)\n",
    "#     dlval = DataLoader(dsval,\n",
    "#                     batch_size = 2,\n",
    "#                     shuffle = False,\n",
    "# #                     sampler = sampler_val,\n",
    "#                     num_workers = n_workers_val,\n",
    "#                     drop_last = False)\n",
    "\n",
    "    ## model\n",
    "    dl = get_camvid_loader.get_data_loader(cfg, mode='train')\n",
    "\n",
    "    dlval = get_camvid_loader.get_data_loader(cfg, mode='val')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ignore_idx = 255\n",
    "    net = BiSeNet(backbone=args.backbone, n_classes=n_classes, pretrain_model=args.pretrain_path, \n",
    "    use_boundary_2=use_boundary_2, use_boundary_4=use_boundary_4, use_boundary_8=use_boundary_8, \n",
    "    use_boundary_16=use_boundary_16, use_conv_last=args.use_conv_last)\n",
    "\n",
    "    if not args.ckpt is None:\n",
    "        net.load_state_dict(torch.load(args.ckpt, map_location='cpu'))\n",
    "    net.cuda()\n",
    "    net.train()\n",
    "#     net = nn.parallel.DistributedDataParallel(net,\n",
    "#             device_ids = [args.local_rank, ],\n",
    "#             output_device = args.local_rank,\n",
    "#             find_unused_parameters=True\n",
    "#             )\n",
    "\n",
    "    score_thres = 0.7\n",
    "    n_min = n_img_per_gpu*cropsize[0]*cropsize[1]//16\n",
    "    criteria_p = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "    criteria_16 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "    criteria_32 = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)\n",
    "    boundary_loss_func = DetailAggregateLoss()\n",
    "    ## optimizer\n",
    "    maxmIOU50 = 0.\n",
    "    maxmIOU75 = 0.\n",
    "    momentum = 0.9\n",
    "    weight_decay = 5e-4\n",
    "    lr_start = 1e-2\n",
    "    max_iter = args.max_iter\n",
    "    save_iter_sep = args.save_iter_sep\n",
    "    power = 0.9\n",
    "    warmup_steps = args.warmup_steps\n",
    "    warmup_start_lr = 1e-5\n",
    "\n",
    "#     if dist.get_rank()==0: \n",
    "#         print('max_iter: ', max_iter)\n",
    "#         print('save_iter_sep: ', save_iter_sep)\n",
    "#         print('warmup_steps: ', warmup_steps)\n",
    "    optim = Optimizer(\n",
    "#             model = net.module,\n",
    "            model = net,\n",
    "            loss = boundary_loss_func,\n",
    "            lr0 = lr_start,\n",
    "            momentum = momentum,\n",
    "            wd = weight_decay,\n",
    "            warmup_steps = warmup_steps,\n",
    "            warmup_start_lr = warmup_start_lr,\n",
    "            max_iter = max_iter,\n",
    "            power = power)\n",
    "    \n",
    "    ## train loop\n",
    "    msg_iter = 50\n",
    "    loss_avg = []\n",
    "    loss_boundery_bce = []\n",
    "    loss_boundery_dice = []\n",
    "    st = glob_st = time.time()\n",
    "    diter = iter(dl)\n",
    "    epoch = 0\n",
    "    for it in range(max_iter):\n",
    "        try:\n",
    "            im, lb = next(diter)\n",
    "            if not im.size()[0]==n_img_per_gpu: raise StopIteration\n",
    "        except StopIteration:\n",
    "            epoch += 1\n",
    "#             sampler.set_epoch(epoch)\n",
    "            diter = iter(dl)\n",
    "            im, lb = next(diter)\n",
    "        im = im.cuda()\n",
    "        lb = lb.cuda()\n",
    "        H, W = im.size()[2:]\n",
    "        lb = torch.squeeze(lb, 1)\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "\n",
    "        if use_boundary_2 and use_boundary_4 and use_boundary_8:\n",
    "            out, out16, out32, detail2, detail4, detail8 = net(im)\n",
    "        \n",
    "        if (not use_boundary_2) and use_boundary_4 and use_boundary_8:\n",
    "            out, out16, out32, detail4, detail8 = net(im)\n",
    "\n",
    "        if (not use_boundary_2) and (not use_boundary_4) and use_boundary_8:\n",
    "            out, out16, out32, detail8 = net(im)\n",
    "\n",
    "        if (not use_boundary_2) and (not use_boundary_4) and (not use_boundary_8):\n",
    "            out, out16, out32 = net(im)\n",
    "\n",
    "            \n",
    "        print('out',out.shape)\n",
    "        print('lb',lb.shape)\n",
    "        \n",
    "        print('out',out.dtype)\n",
    "        print('lb',lb.dtype)\n",
    "        \n",
    "        lossp = criteria_p(out, lb.long())\n",
    "        loss2 = criteria_16(out16, lb.long())\n",
    "        loss3 = criteria_32(out32, lb.long())\n",
    "        \n",
    "        boundery_bce_loss = 0.\n",
    "        boundery_dice_loss = 0.\n",
    "        \n",
    "        \n",
    "        if use_boundary_2: \n",
    "            # if dist.get_rank()==0:\n",
    "            #     print('use_boundary_2')\n",
    "            boundery_bce_loss2,  boundery_dice_loss2 = boundary_loss_func(detail2, lb)\n",
    "            boundery_bce_loss += boundery_bce_loss2\n",
    "            boundery_dice_loss += boundery_dice_loss2\n",
    "        \n",
    "        if use_boundary_4:\n",
    "            # if dist.get_rank()==0:\n",
    "            #     print('use_boundary_4')\n",
    "            boundery_bce_loss4,  boundery_dice_loss4 = boundary_loss_func(detail4, lb)\n",
    "            boundery_bce_loss += boundery_bce_loss4\n",
    "            boundery_dice_loss += boundery_dice_loss4\n",
    "\n",
    "        if use_boundary_8:\n",
    "            # if dist.get_rank()==0:\n",
    "            #     print('use_boundary_8')\n",
    "            boundery_bce_loss8,  boundery_dice_loss8 = boundary_loss_func(detail8, lb)\n",
    "            boundery_bce_loss += boundery_bce_loss8\n",
    "            boundery_dice_loss += boundery_dice_loss8\n",
    "\n",
    "        loss = lossp + loss2 + loss3 + boundery_bce_loss + boundery_dice_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loss_avg.append(loss.item())\n",
    "\n",
    "        loss_boundery_bce.append(boundery_bce_loss.item())\n",
    "        loss_boundery_dice.append(boundery_dice_loss.item())\n",
    "\n",
    "        ## print training log message\n",
    "        if (it+1)%msg_iter==0:\n",
    "            loss_avg = sum(loss_avg) / len(loss_avg)\n",
    "            lr = optim.lr\n",
    "            ed = time.time()\n",
    "            t_intv, glob_t_intv = ed - st, ed - glob_st\n",
    "            eta = int((max_iter - it) * (glob_t_intv / it))\n",
    "            eta = str(datetime.timedelta(seconds=eta))\n",
    "\n",
    "            loss_boundery_bce_avg = sum(loss_boundery_bce) / len(loss_boundery_bce)\n",
    "            loss_boundery_dice_avg = sum(loss_boundery_dice) / len(loss_boundery_dice)\n",
    "            msg = ', '.join([\n",
    "                'it: {it}/{max_it}',\n",
    "                'lr: {lr:4f}',\n",
    "                'loss: {loss:.4f}',\n",
    "                'boundery_bce_loss: {boundery_bce_loss:.4f}',\n",
    "                'boundery_dice_loss: {boundery_dice_loss:.4f}',\n",
    "                'eta: {eta}',\n",
    "                'time: {time:.4f}',\n",
    "            ]).format(\n",
    "                it = it+1,\n",
    "                max_it = max_iter,\n",
    "                lr = lr,\n",
    "                loss = loss_avg,\n",
    "                boundery_bce_loss = loss_boundery_bce_avg,\n",
    "                boundery_dice_loss = loss_boundery_dice_avg,\n",
    "                time = t_intv,\n",
    "                eta = eta\n",
    "            )\n",
    "            \n",
    "            logger.info(msg)\n",
    "            loss_avg = []\n",
    "            loss_boundery_bce = []\n",
    "            loss_boundery_dice = []\n",
    "            st = ed\n",
    "            # print(boundary_loss_func.get_params())\n",
    "        if (it+1)%save_iter_sep==0:# and it != 0:\n",
    "            \n",
    "            ## model\n",
    "            logger.info('evaluating the model ...')\n",
    "            logger.info('setup and restore model')\n",
    "            \n",
    "            net.eval()\n",
    "\n",
    "            # ## evaluator\n",
    "            logger.info('compute the mIOU')\n",
    "            with torch.no_grad():\n",
    "                single_scale1 = MscEvalV0()\n",
    "                mIOU50 = single_scale1(net, dlval, n_classes)\n",
    "\n",
    "                single_scale2= MscEvalV0(scale=0.75)\n",
    "                mIOU75 = single_scale2(net, dlval, n_classes)\n",
    "\n",
    "\n",
    "            save_pth = osp.join(save_pth_path, 'model_iter{}_mIOU50_{}_mIOU75_{}.pth'\n",
    "            .format(it+1, str(round(mIOU50,4)), str(round(mIOU75,4))))\n",
    "            \n",
    "            state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n",
    "#             if dist.get_rank()==0: \n",
    "            torch.save(state, save_pth)\n",
    "\n",
    "            logger.info('training iteration {}, model saved to: {}'.format(it+1, save_pth))\n",
    "\n",
    "            if mIOU50 > maxmIOU50:\n",
    "                maxmIOU50 = mIOU50\n",
    "                save_pth = osp.join(save_pth_path, 'model_maxmIOU50.pth'.format(it+1))\n",
    "                state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n",
    "                if dist.get_rank()==0: \n",
    "                    torch.save(state, save_pth)\n",
    "                    \n",
    "                logger.info('max mIOU model saved to: {}'.format(save_pth))\n",
    "            \n",
    "            if mIOU75 > maxmIOU75:\n",
    "                maxmIOU75 = mIOU75\n",
    "                save_pth = osp.join(save_pth_path, 'model_maxmIOU75.pth'.format(it+1))\n",
    "                state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n",
    "                if dist.get_rank()==0: torch.save(state, save_pth)\n",
    "                logger.info('max mIOU model saved to: {}'.format(save_pth))\n",
    "            \n",
    "            logger.info('mIOU50 is: {}, mIOU75 is: {}'.format(mIOU50, mIOU75))\n",
    "            logger.info('maxmIOU50 is: {}, maxmIOU75 is: {}.'.format(maxmIOU50, maxmIOU75))\n",
    "\n",
    "            net.train()\n",
    "    \n",
    "    ## dump the final model\n",
    "    save_pth = osp.join(save_pth_path, 'model_final.pth')\n",
    "    net.cpu()\n",
    "    state = net.module.state_dict() if hasattr(net, 'module') else net.state_dict()\n",
    "    if dist.get_rank()==0: torch.save(state, save_pth)\n",
    "    logger.info('training done, model saved to: {}'.format(save_pth))\n",
    "    print('epoch: ', epoch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_thres = 0.7\n",
    "n_min = 8*720*960//16\n",
    "criteria_p = OhemCELoss(thresh=score_thres, n_min=n_min, ignore_lb=ignore_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(8,19,720,960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(8,720,960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criteria_p(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = get_camvid_loader.get_data_loader(cfg, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diter = iter(dl)\n",
    "# epoch = 0\n",
    "for it in range(1):\n",
    "#     try:\n",
    "#         im, lb = next(diter)\n",
    "#         if not im.size()[0]==n_img_per_gpu: raise StopIteration\n",
    "#     except StopIteration:\n",
    "#         epoch += 1\n",
    "# #             sampler.set_epoch(epoch)\n",
    "    diter = iter(dl)\n",
    "    im, lb = next(diter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,data in enumerate(dl):\n",
    "    img,label = data\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install thop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nvidia-pyindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nvidia-tensorrt==7.1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = ('b', 3,'w','h')\n",
    "input_2= ('b', 4)\n",
    "output= ('b', 'd', 5)\n",
    "torch.onnx.export(model, args, f, export_params=True, verbose=False, training=False, input_names=None, output_names=None, dynamic_axes = {'input_1':[0, 2, 3], 'input_2':[0], 'output':[0, 1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ar = np.ones((2,3,4))\n",
    "print(ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from loss.util import enet_weighing\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OhemCELoss(nn.Module):\n",
    "    def __init__(self, thresh, n_min, ignore_lb=255, *args, **kwargs):\n",
    "        super(OhemCELoss, self).__init__()\n",
    "        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).cuda()\n",
    "        self.n_min = n_min\n",
    "        self.ignore_lb = ignore_lb\n",
    "        self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        N, C, H, W = logits.size()\n",
    "        loss = self.criteria(logits, labels).view(-1)\n",
    "        loss, _ = torch.sort(loss, descending=True)\n",
    "        if loss[self.n_min] > self.thresh:\n",
    "            loss = loss[loss>self.thresh]\n",
    "        else:\n",
    "            loss = loss[:self.n_min]\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class WeightedOhemCELoss(nn.Module):\n",
    "    def __init__(self, thresh, n_min, num_classes, ignore_lb=255, *args, **kwargs):\n",
    "        super(WeightedOhemCELoss, self).__init__()\n",
    "        self.thresh = -torch.log(torch.tensor(thresh, dtype=torch.float)).cuda()\n",
    "        self.n_min = n_min\n",
    "        self.ignore_lb = ignore_lb\n",
    "        self.num_classes = num_classes\n",
    "        # self.criteria = nn.CrossEntropyLoss(ignore_index=ignore_lb, reduction='none')\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        N, C, H, W = logits.size()\n",
    "        criteria = nn.CrossEntropyLoss(weight=enet_weighing(labels, self.num_classes).cuda(), ignore_index=self.ignore_lb, reduction='none')\n",
    "        loss = criteria(logits, labels).view(-1)\n",
    "        loss, _ = torch.sort(loss, descending=True)\n",
    "        if loss[self.n_min] > self.thresh:\n",
    "            loss = loss[loss>self.thresh]\n",
    "        else:\n",
    "            loss = loss[:self.n_min]\n",
    "        return torch.mean(loss)\n",
    "\n",
    "class SoftmaxFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma, ignore_lb=255, *args, **kwargs):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.nll = nn.NLLLoss(ignore_index=ignore_lb)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        scores = F.softmax(logits, dim=1)\n",
    "        factor = torch.pow(1.-scores, self.gamma)\n",
    "        log_score = F.log_softmax(logits, dim=1)\n",
    "        log_score = factor * log_score\n",
    "        loss = self.nll(log_score, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(15)\n",
    "    criteria1 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n",
    "    criteria2 = OhemCELoss(thresh=0.7, n_min=16*20*20//16).cuda()\n",
    "    net1 = nn.Sequential(\n",
    "        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n",
    "    )\n",
    "    net1.cuda()\n",
    "    net1.train()\n",
    "    net2 = nn.Sequential(\n",
    "        nn.Conv2d(3, 19, kernel_size=3, stride=2, padding=1),\n",
    "    )\n",
    "    net2.cuda()\n",
    "    net2.train()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inten = torch.randn(16, 3, 20, 20).cuda()\n",
    "        lbs = torch.randint(0, 19, [16, 20, 20]).cuda()\n",
    "        lbs[1, :, :] = 255\n",
    "\n",
    "    logits1 = net1(inten)\n",
    "    logits1 = F.interpolate(logits1, inten.size()[2:], mode='bilinear')\n",
    "    logits2 = net2(inten)\n",
    "    logits2 = F.interpolate(logits2, inten.size()[2:], mode='bilinear')\n",
    "    \n",
    "    print('logits1',logits1.dtype)\n",
    "    print('lbs',lbs.dtype)\n",
    "    \n",
    "    loss1 = criteria1(logits1, lbs)\n",
    "    loss2 = criteria2(logits2, lbs)\n",
    "    loss = loss1 + loss2\n",
    "    print(loss.detach().cpu())\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspth = '/xiaoou/my_last_project/dataset/cityscapes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = args.mode\n",
    "cropsize = [512,1024]\n",
    "randomscale = (0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0, 1.125, 1.25, 1.375, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CityScapes(dspth, cropsize=cropsize, mode=mode, randomscale=randomscale)\n",
    "#     sampler = torch.utils.data.distributed.DistributedSampler(ds)\n",
    "dl = DataLoader(ds,\n",
    "                batch_size = n_img_per_gpu,\n",
    "                shuffle = False,\n",
    "#                     sampler = ds,\n",
    "                num_workers = n_workers_train,\n",
    "                pin_memory = False,\n",
    "                drop_last = True)\n",
    "# exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    dsval = CityScapes(dspth, mode='val', randomscale=randomscale)\n",
    "#     sampler_val = torch.utils.data.distributed.DistributedSampler(dsval)\n",
    "    dlval = DataLoader(dsval,\n",
    "                    batch_size = 2,\n",
    "                    shuffle = False,\n",
    "#                     sampler = sampler_val,\n",
    "                    num_workers = n_workers_val,\n",
    "                    drop_last = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
